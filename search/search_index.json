{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"_draft/Git%20worktrees/","tags":["git"],"text":"Switching Development Contexts with Git \u00b6 Context \u00b6 On my daily work I have to switch context while in the middle of a modification many times, or I just have to work on two different versions in parallel. Working with git there are many ways to save your work and come back later, but I have found all of them too cumbersome in the process to resume work. I do not need this, I already have stash to save my work! \u00b6 Yeah you can stash your work and checkout a new branch but stash is not very user friendly and if you spend too much time in your new task you will have a hard time remembering wich stash element was the good one unless you have a really strict naming policy while stashig things \u2026 which, to he honest most of us do not have in our daily work, so let alone when we are in a hurry. Well, I really do NOT need this cause I use WIP commits \u2026 \u00b6 Sure, you can commit your progress with some kind of wip label and checkout a new branch but that would mostly result in you having to move changes around between commits to tidy up your changes in small and meaningfull commits before committing the final work \u2026 and still there is the problem with checkout \u00b6 Both of the above systems rely in save your work somehow and checkout a new branch to star working in the new urgent task. That usually works fine but \u2026 what if your new branch is an old maintenance branch and your project layout or your build system had changed \u2026 ? Well your IDE will probably freak out and you will end having to deal with a bunch of errors in your indexer, openned files which do not exist anymore \u2026 you probably know what I mean How worktree works \u00b6 worktree is a git command. It allows you to create more than one working tree for each repository. Normally each repo has one main working tree (maybe zero on bare repos but they are out of scope here), and zero or more linked working trees. worktree will allow you to checkout a new or already existing branch in a new folder, that way you can switch between them with zero effort. \u2026 but I already can clone the repo multiple times, so no big deal! \u00b6 Ok, yes. You already can clone a git repo on different folders and you would have something very similar to the scenario above. In fact that was my former approach to the problem. So what makes learning to use worktree worthy? \u00b6 Well, first of all make a new repo clone takes disk space, sometimes a lot of disk space. As worktrees are not new clones but just linked workingtrees of a main repo, they share most of the \u201cadministration\u201d info with your main working tree. Another improvement of worktree over multiple clones is in fact a \u201climitation\u201d that worktree impose on us but that I have found quite useful to avoid errors. worktree does NOT allow us to have more than one workingtree pointing to the same branch at the same time. This will help us to avoid headaches having more than one way to modify the content of a branch. Finally, being a git command worktree provides useful commands to list , add and delete workingtrees and so having them under control from a central point. Some IDEs already have support for working trees also. Use cases \u00b6 I have found worktree command most useful when: Working on two different PRs and have to switch between then to address comments I have to solve bugs or test something on old maintenance versions Brief commands reference \u00b6 Create a new working tree and a new branch from the current point. \u00b6 git worktree add -b <new_branch_name> <new_folder_path> Create a new working tree from an existing branch \u00b6 git worktree add <new_folder_path> <existing_branch_name> List all the existing working trees \u00b6 git worktree list Links and more info \u00b6 Official docs for worktree : https://git-scm.com/docs/git-worktree/2.35.0 An StackOverflow thread with some additional info regarding commands https://stackoverflow.com/a/45491767 Bonus track: How I layout my workingtrees \u00b6 Create a new folder with the project name $> mkdir awesomeProject Clone the main (or master if you still live in the old good days) in its own folder $> cd awesomeProject awesomeProject$> git clone git@github.com:jlojosnegros/awesome-project.git main From there you could create your new workingtrees to work on your features using sibling folders awesomeProject$> cd main awesomeProject/main [ main { origin/main }] $> git worktree list ~/awesomeProject/main d4f9c34 [ main ] awesomeProject/main [ main { origin/main }] $> git worktree add -b newAwesomeFeature ../new-awesome-feature Preparing worktree ( new branch 'newAwesomeFeature' ) HEAD is now at d4f9c34 My first and last commit awesomeProject/main [ main { origin/main }] $> git worktree list ~/awesomeProject/main d4f9c34 [ main ] ~/awesomeProject/new-awesome-feature d4f9c34 [ newAwesomeFeature ] awesomeProject/main [ main { origin/main }] $> cd ../new-awesome-feature awesomeProject/new-awesome-feature [ newAwesomeFeature { L }] $> TL;DR \u00b6 TBD","title":"Switching Development Contexts with Git"},{"location":"_draft/Git%20worktrees/#switching-development-contexts-with-git","text":"","title":"Switching Development Contexts with Git"},{"location":"_draft/Git%20worktrees/#context","text":"On my daily work I have to switch context while in the middle of a modification many times, or I just have to work on two different versions in parallel. Working with git there are many ways to save your work and come back later, but I have found all of them too cumbersome in the process to resume work.","title":"Context"},{"location":"_draft/Git%20worktrees/#i-do-not-need-this-i-already-have-stash-to-save-my-work","text":"Yeah you can stash your work and checkout a new branch but stash is not very user friendly and if you spend too much time in your new task you will have a hard time remembering wich stash element was the good one unless you have a really strict naming policy while stashig things \u2026 which, to he honest most of us do not have in our daily work, so let alone when we are in a hurry.","title":"I do not need this, I already have stash to save my work!"},{"location":"_draft/Git%20worktrees/#well-i-really-do-not-need-this-cause-i-use-wip-commits","text":"Sure, you can commit your progress with some kind of wip label and checkout a new branch but that would mostly result in you having to move changes around between commits to tidy up your changes in small and meaningfull commits before committing the final work","title":"Well, I really do NOT need this cause I use WIP commits ..."},{"location":"_draft/Git%20worktrees/#and-still-there-is-the-problem-with-checkout","text":"Both of the above systems rely in save your work somehow and checkout a new branch to star working in the new urgent task. That usually works fine but \u2026 what if your new branch is an old maintenance branch and your project layout or your build system had changed \u2026 ? Well your IDE will probably freak out and you will end having to deal with a bunch of errors in your indexer, openned files which do not exist anymore \u2026 you probably know what I mean","title":"... and still there is the problem with checkout"},{"location":"_draft/Git%20worktrees/#how-worktree-works","text":"worktree is a git command. It allows you to create more than one working tree for each repository. Normally each repo has one main working tree (maybe zero on bare repos but they are out of scope here), and zero or more linked working trees. worktree will allow you to checkout a new or already existing branch in a new folder, that way you can switch between them with zero effort.","title":"How worktree works"},{"location":"_draft/Git%20worktrees/#but-i-already-can-clone-the-repo-multiple-times-so-no-big-deal","text":"Ok, yes. You already can clone a git repo on different folders and you would have something very similar to the scenario above. In fact that was my former approach to the problem.","title":"... but I already can clone the repo multiple times, so no big deal!"},{"location":"_draft/Git%20worktrees/#so-what-makes-learning-to-use-worktree-worthy","text":"Well, first of all make a new repo clone takes disk space, sometimes a lot of disk space. As worktrees are not new clones but just linked workingtrees of a main repo, they share most of the \u201cadministration\u201d info with your main working tree. Another improvement of worktree over multiple clones is in fact a \u201climitation\u201d that worktree impose on us but that I have found quite useful to avoid errors. worktree does NOT allow us to have more than one workingtree pointing to the same branch at the same time. This will help us to avoid headaches having more than one way to modify the content of a branch. Finally, being a git command worktree provides useful commands to list , add and delete workingtrees and so having them under control from a central point. Some IDEs already have support for working trees also.","title":"So what makes learning to use worktree worthy?"},{"location":"_draft/Git%20worktrees/#use-cases","text":"I have found worktree command most useful when: Working on two different PRs and have to switch between then to address comments I have to solve bugs or test something on old maintenance versions","title":"Use cases"},{"location":"_draft/Git%20worktrees/#brief-commands-reference","text":"","title":"Brief commands reference"},{"location":"_draft/Git%20worktrees/#create-a-new-working-tree-and-a-new-branch-from-the-current-point","text":"git worktree add -b <new_branch_name> <new_folder_path>","title":"Create a new working tree and a new branch from the current point."},{"location":"_draft/Git%20worktrees/#create-a-new-working-tree-from-an-existing-branch","text":"git worktree add <new_folder_path> <existing_branch_name>","title":"Create a new working tree from an existing branch"},{"location":"_draft/Git%20worktrees/#list-all-the-existing-working-trees","text":"git worktree list","title":"List all the existing working trees"},{"location":"_draft/Git%20worktrees/#links-and-more-info","text":"Official docs for worktree : https://git-scm.com/docs/git-worktree/2.35.0 An StackOverflow thread with some additional info regarding commands https://stackoverflow.com/a/45491767","title":"Links and more info"},{"location":"_draft/Git%20worktrees/#bonus-track-how-i-layout-my-workingtrees","text":"Create a new folder with the project name $> mkdir awesomeProject Clone the main (or master if you still live in the old good days) in its own folder $> cd awesomeProject awesomeProject$> git clone git@github.com:jlojosnegros/awesome-project.git main From there you could create your new workingtrees to work on your features using sibling folders awesomeProject$> cd main awesomeProject/main [ main { origin/main }] $> git worktree list ~/awesomeProject/main d4f9c34 [ main ] awesomeProject/main [ main { origin/main }] $> git worktree add -b newAwesomeFeature ../new-awesome-feature Preparing worktree ( new branch 'newAwesomeFeature' ) HEAD is now at d4f9c34 My first and last commit awesomeProject/main [ main { origin/main }] $> git worktree list ~/awesomeProject/main d4f9c34 [ main ] ~/awesomeProject/new-awesome-feature d4f9c34 [ newAwesomeFeature ] awesomeProject/main [ main { origin/main }] $> cd ../new-awesome-feature awesomeProject/new-awesome-feature [ newAwesomeFeature { L }] $>","title":"Bonus track: How I layout my workingtrees"},{"location":"_draft/Git%20worktrees/#tldr","text":"TBD","title":"TL;DR"},{"location":"pages/kubernetes/architecture/k8s/","tags":["kubernetes"],"text":"Arquitectura de Kubernetes \u00b6 kubernetes \u00b6 Componentes \u00b6 Control Plane \u00b6 Aqui es donde est\u00e1n los elementos que manejan el comportamiento del cluster pero que no ejecutan nada de las aplicaciones de los containers. Componentes \u00b6 etcd API Server #api-server Scheduler Controller Manager The Nodes ( workers ) \u00b6 Aqui es donde se ejecutan los containers de la aplicaci\u00f3n. Componentes \u00b6 Kubelet Kubernetes Service Proxy (kube-proxy) Conainer Runtime (Docker, rkt \u2026 ) Add-on Components \u00b6 A parte de los componentes del Control Plane y de los nodos se necesitan unas cuantas cosas para que todo funcione bien Kubernetes DNS Server Dashboard Ingress Controller Heapster Container Network Interface network plugin Como se comunican \u00b6 Todos se comunican con el API server. El API server es el \u00fanico que se comunica con la base de datos. Todos modifican el estado de la base de datos hablando con el API server. La comunicaci\u00f3n es siempre comenzada por los otros elementos salvo en algunos casos especiales donde el API server es quien comienza la conexion. Cuando usamos kubectl para obtener los logs Cuando usamos kubectl attach Cuando usamos kubectl port-forward Ejecutando multiples instancias. \u00b6 Todos los elementos que tienen los Worker Nodes tienen que correr en el mismo worker node. ( supongo que en cada nodo tienen que estar todos los elementos) Sin embargo los elementos del control plane pueden ser ejecutados en distintos servidores. Se pueden ejecutar multiples instancias de los elementos del Control plane para tener HA. Sin embargo: Etcd y API server pueden tener varias instancias funcionando en paralelo Scheduler y Controller Manager solo pueden tener una instancia funcionando al mismo tiempo mientras que las dem\u00e1s est\u00e1n en stand-by Como se ejecutan los componentes \u00b6 Todos los componentes del control plane y el kube-proxy pueden ejecutarse en el sistema directamente o pueden ejecutarse como pods en el master node . El unico que tiene que ejecutarse como un componente de sistema siempre es Kubelet Como se usan los componentes \u00b6 Como utiliza K8s ETCD \u00b6 Es el \u00fanico sitio donde se guardan los manifiestos de todos los componentes que se crean en el cluster. Es una base de datos clave-valor distribuida, lo que permite poder tener multiples instancias para tener HA El unico que se comunica con el etcd es el API Server. Esto tiene unas ventajas Se puede cambiar la base de datos con facilidad adaptando el API server. Puede implementarse un sistema de locking optimista Se puede hacer validacion de los elementos antes de guardarlos o de recuperarlos. Como se guardan las cosas en etcd \u00b6 Hay dos versiones en uso v2 y v3 ( aunque seguramente ahora la v2 ya no se use porque la v3 tiene mejor performance.) En v2 Las claves son jerarquicas y estan guardadas en arbol, por lo que puede pensarse en la clave de una entrada como en el nombre completo de un fichero en un arbol de directorios. Por tanto cada clave es: O bien un directorio que a su vez contiene otras claves O bien una clave regular que contiene un valor. En v3 No soporta el concepto de directorios per se, pero como las claves siguen siendo organizadas de manera jer\u00e1rquica y se pueden poner / , podemos seguir pensando en ellas como el nombre completo de un fichero dentro de un filesystem. Todos los datos que kubernetes guarda en etcd se guardan debajo de /registry title: Listado de entradas en etcd V2 ```bash $ etcdctl ls /registry ``` V3: Las entradas cuya clave empieza por un prefijo determinado. ```bash $ etcdtl get /registry --prefix=true ``` Salida ```bash /registry/configmaps /registry/daemonsets /registry/deployments /registry/events /registry/namespaces /registry/pods ... ``` title : pods en el namespace default collapse: true ```bash $ etcdctl ls /registry/pods/default /registry/pods/default/kubia-159041347-xk0vc /registry/pods/default/kubia-159041347-wt6ga /registry/pods/default/kubia-159041347-hp2o5 ``` y esto es lo que se guarda de una entrada en particular ```bash $ etcdctl get /registry/pods/default/kubia-159041347-wt6ga {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\", \"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":... ``` Como puede verse es un documento en formato json. Consistencia en ETCD cuando hay un cluster. \u00b6 Como podemos tener multiples instancias de etcd en un cluster tenemos que asegurar que hay consistencia en el estado que muestran todas. Para esto se utiliza el [[Algoritmo RAFT]] Es un algoritmo de consenso, de manera que para transicionar a un nuevo estado, mas de la mitad de los nodos tienen que estar de acuerdo. En este algoritmo se asegura que una determinada instancia de etcd o bien muestra el ultimo estado al que se ha llegado o bien muestra uno de los antiguos estados a los que se llego por consenso. Si se produce un split brain, si una de las particiones tiene la mayoria de los nodos evidentemente la otra no los tendr\u00e1. Por tanto la primera particion si que podr\u00e1 transicionar de estado porque tienen la mayoria, mientras que la otra no podr\u00e1 modificar el estado del cluster, qued\u00e1ndose en un estado v\u00e1lido., Cuando la situacion se solucione los nodos que estaban en un estado antiguo, pero valido, podr\u00e1n recuperarse y hacer catch-up. Debido a la necesidad de quorum es mejor siempre tener un numero impar de instancias de etcd. Teniendo un numero par no aumentamos el numero de fallos de instancias a los que somos resilientes, pero si que aumentamos la posibilidad de fallo, ya que a mas instancias en el cluster mas posibilidades de que alguna falle. Normalmente los clusters mas grandes de etcd tienen CINCO o SIETE instancias, lo que permite manejar sin problemas el fallo de DOS o TRES instancias, lo que suele ser suficiente en la mayoria de los casos. Que hace el API server \u00b6 Intro \u00b6 Es el componente que todos los demas componentes, incluido el kubectl utilizan para poder interactuar con el sistema. Provee un interface CRUD sobre un API Rest Guarda todos los datos en ETCD Provee consistencia en los datos que se guardan Chequea los datos que los demas componentes quieren guardar en etcd antes de guardarlos para evitar estados inconsistentes del sistema Implementa un Optimistic Locking system Dado que podemos tener multiples instancias de API server ejecutandose de manera concurrente tenemos que evitar que los cambios puedan sobreescribir otros cambios que se est\u00e9n haciendo en ese momento 'kubectl' es uno de los clientes del API server. Cuando creamos un nuevo recurso con un fichero `yaml` lo que hace es enviar un HTTP POST con el fichero en formato json y esto es lo que pasa en el API Server ![[APIServerHTTPPOST.png]] Stages \u00b6 Authentication \u00b6 Lo primero que se hace es autenticar al cliente que envia la peticion. Se envia la peticion a una lista de plugins de autenticacion que esten configurados en el API server, pasando por todos hasta que uno de ellos sea capaz de determinar quien es el cliente que envia la peticion. Authorization \u00b6 Una vez que sabemos quien es el cliente que pide la accion y que podemos confiar en el, tenemos que ver si dicho cliente tiene autorizacion para poder hacer dicha accion sobre el recurso que esta pidiendo hacerla. Para esto estan los plugins de autorizacion. La peticion va pasando por los plugins hasta que uno de ellos dice que el cliente puede realizar al accion sobre el recurso pedido. Admision Control \u00b6 Las operaciones de Creacion, Modificacion y Borrado de recursos pasan por una fase adicional, la de Control de Admision. En esta fase la peticion pasas por todos los plugins . Estos plugins pueden modificar la peticion A\u00f1adiendo valores por defecto que no existen en el recurso de la peticion Cambiando valores del recursos que exceden limites Incluso pueden modificar recursos relacionados que no est\u00e9n en la peticion Tambien pueden validad y rechazar la peticion por el contenido que tiene. title: Ejemplos de Admission Control Plugins. - `AlwaysPullImages` -> Sobreescribe el `imagePullPolicy` de los Pods y lo pone a `Always` - `ServiceAccount` -> Aplica un `ServiceAccount` por defecto a todos los Pods que no definen uno. - `NamespaceLifecycle` -> Evita que se puedan crear Pods en namespaces que est\u00e1n en proceso de borrado o que no existen. - `ResourceQuota` -> Se asegura que los Pods de un determinado namespace solo puedan utilizar unas cantidades de CPU y memoria determinadas How API Server Notifies clients. \u00b6 El API server permite que los demas componentes pidan ser notificados cuando un recurso es Creado Modificado o Borrado, de manera que pueda hacer lo que sea que tenga que hacer en respuesta a esta operacion. El Cliente abre una conexion HTTP al API server y a trav\u00e9s de dicha conexion el cliente recive un stream de moficiaciones. Cada vez que uno de los objetos observados cambia se le envia al cliente la nueva version del objeto. Este mecanismo lo usa por ejemplo el kubectl cuando pedimos que nos haga un watch de los pods kubectl get pods -o yaml --watch Tambien lo utiliza el Scheduler Entendiendo el funcionamiento del Scheduler \u00b6 Controllers ejecutandose en el Controller Manager. \u00b6 Intro \u00b6 Hasta ahora tenemos en el control plane: ETCD: que solo guarda los manifiestos. API Server: que es un interface para que los demas puedan guardar cosas en la base de datos y ser notificados Scheduler: Que se encarga de asingar nodos a los nuevos pods. Pero no tenemos nadie que haga nada para mantener el estado del cluster conciliado con el estado de los manifiestos guardados en la base de datos. Esto es lo que hace el Controller Manager, o para ser mas exactlos la lista de controllers que se ejecutan en el. Todos los controller se al formar parte del Control Plane, se ejecutan en el Master Node Controllers en general \u00b6 Actualmente ( no tengo claro que eso sea cierto ahora mismo) un solo proceso Controller Manager combina dentro de el varios controllers distintos que realizan distintas labores. Eventualmente cada uno de estos controllers ser\u00e1 separado en un proceso distinto para que sea mas sencillo cambiarlos. Lista de Controllers: Replication Manager \u2013 Un controller para ReplicationController resources ReplicaSet Controller DaemonSet Controller Job Controller Deployment Controller StatefulSet Controller Node Controller Service Controller Endpoints Controller Namespace Controller PersistentVolumeController Others \u2026 que cachondo. Basicamente hay un Controller por cada uno de los recursos de kubernetes. Los recursos son una declaracion de cual queremos que sea el estado de un cluster. Los Controllers realizan el trabajo de intentar conciliar el estado actual del cluster con el que queremos que sea. En general los controllers tienen un \u201creconciliation loop\u201d donde intentan hacer esta reconciliacion del estado creando, modificando o borrando determinados recursos. collapse:true En cada recurso el `spec` es donde ponemos el estado esperado mientras que los controllers escriben en el `status` el estado actual. Utilizan el sistema de notificacion del API server para saber cuando se modifican los recursos a los que atienden. Tambien tienen una operacion \u201cre-list\u201d que ejecutan periodicamente por si acaso han perdido alguna notificacion. Los Controllers no hablan nunca unos con otros. Los Controllers no saben ni que existen otros controllers. Solo hablan con el API server nada mas. Cada uno hace su parte y esperan que todo funcione correctamente. Algunos controllers \u00b6 Replication Manager \u00b6 Basicamente espera ser notificado de los Pods y los ReplicationController que se van creando, destruyendo o modificando para comprobar si lo que se ha puesto en los ReplicationController se sigue cumpliendo. En el caso de que el n\u00famero de pods no sea el esperado realiza las operaciones adecuadas para poder llegar al estado deseado. Si necesita crear nuevos pods utiliza el spec.template del ReplicationController para enviar un HTTP POST al API Server con el nuevo manifiesto para que se desencadene la creacion de un nuevo pod. El Replication Manager hace su trabajo solo modificando los manifiestos de los recursos mediante el API server como hacen todos los demas controllers ReplicaSet Controller, DaemonSet Controller y Job Controller \u00b6 Los tres hacen basicamente lo mismo que el Replication Manager, solo que usando distintos recursos\u2026 Kubernetes ReplicaSet , Kubernetes DaemonSet y Kubernetes Job Deployment Controller \u00b6 Gestiona las actualizaciones de los Deployment . Hace un roll-out del deployment cuando se modifica. Basicamente crea un nuevo Replicaset y despues realiza una escalacion tanto del antiguo como del nuevo Replicaset basandose en la estrategia definida en el Deployment hasta que todos los viejos Pods han desaparecido y han sido reemplazados por los nuevos. StatefulSet Controller \u00b6 Como los demas Controllers el StatefulSet Controller crea y destruye pods de acuerdo con la especificacion de los StatefulSet . Adem\u00e1s es el \u00fanico que tambien maneja los PersistentVolumeClaims para cada instancia de Pod Node Controller \u00b6 Maneja los Node Resources que describen los workers de un cluster. Entre otras cosas mantiene una lista actualizada de los nodos del cluster monitorizando las maquinas que se a\u00f1aden o quitan. Tambien monitoriza la salud de los nodos y elimina los Pods de los nodos que no pueden ser alcanzados. Service Controller \u00b6 Cuando un Kubernetes Service de tipo LoadBalancer se crea se pide un LoadBalancer al sistema para poder hacer el Servicio visible desde el exterior. Este controler es el encargado de realizar la operacion de pedir el recurso y de liberarlo cuando no es necesario. Endpoints Controller \u00b6 Los Kubernetes Service no son enlazados directamente a los Pods, si no que contienen una lista de los endpotins (IP + Puerto), lista que es actualizada de acuerdo con el pod selector del Kubernetes Service El Endpoints Controller es el encargado de mantener la lista de endpoints constantemente actualizada con las IPs y puertos de los Pods que matchean el label selector Namespace Controller \u00b6 Cuando un namespace es borrado todos los recursos que pertenezcan a dicho namespace deben ser borrados. Esto es lo que hace el controller. PersistentVolume Controller \u00b6 Cuando un usuario crea un Kubernetes PersistentVolumeClaims kubernetes tiene que encontrar el Kubernetes PersistentVolume al que poder enlazarlo. Esto es justamente lo que hace este controller. Cuando se crea un nuevo Kubernetes PersistentVolumeClaims este controller intenta encontrar el mejor match para el. Busca el Kubernetes PersistentVolume mas peque\u00f1o que tenga el modo de acceso adecuado y que tenga una capacidad suficiente para el claim. Para esto mantiene una lista ordenada de Kubernetes PersistentVolume para cada uno de los modos de acceso por orden ascendente de capacidad ( asi que solo tiene que devolver el primero de la lista.) Cuando el Kubernetes PersistentVolumeClaims es borrado el Kubernetes PersistentVolume es reclamado por el controller de acuerdo con la politica de reclaim \u2026 ( puede quedarse como est\u00e1, ser borrado, ser vaciado\u2026) Kubelet \u00b6 Intro \u00b6 Al contrario que los controllers tango kubelet como kube-proxy se ejecutan en los worker nodes, que es donde se ejecutan los containers de las aplicaciones. Responsabilidades \u00b6 Kubelet es el responsable de todo lo que sucede en un worker node. Lo primero que tiene que hacer es registrar el nodo en el cluster, para eso crea un Kubernetes Node Resources usando el API Server. A partir de ese momento monitoriza todos los Pods para ver cuales son asignados por el Scheduler para ser ejecutados en su nodo, y cuando esto sucede habla con el Container Runtime que haya configurado para que se ejecuten los containers del Pod. Tambi\u00e9n monitoriza constantemente los containers de los Pods que est\u00e1n ejecutandose en su Nodo para poder reportar su estatus, eventos y consumo de recursos al API Server. Kubelet tambien es el responsable de ejecutar los liveness probes, rearrancar los containers cuando la probe falle y por \u00faltimo ser quien borre los contenedores cuando el Pod sea borrado. Ejecutando Pods staticos sin el API Server. \u00b6 Normalmente kubelet obtiene los manifiestos de los pods que tiene que ejecutar del API server, pero tambien puede ejecutar otros Pods obteniendo los manifiestos de un directorio del Nodo en el que se ejecuta. Esto se utiliza principalmente para poder ejecutar componentes del Control Plane como Pods en lugar de como procesos del master node. Kubernetes Service Proxy \u00b6 Los Worker Nodes tambien ejecutan el kube-proxy. El \u00fanico cometido de kube-proxy es asegurarse que cuando llamamos a uno de los Kubernetes Service la llamada termina en uno de los Pods que est\u00e1 dando soporte a dicho servicio. Cuando hay mas de un Pod dando soporte a dicho servicio se realiza un load-balancing. Se llama kube-proxy porque en la primera de las implementaciones era un proceso en espacio de usuario que hab\u00eda modificado las IP tables para que las peticiones a los servicios fueran redirigidas a \u00e9l y luego fuese el quien las reenviase a los Pods adecuados. Todas las conexiones pasaban por \u00e9l. Ahora hay una implementaci\u00f3n mucho mejor en performance que simplemente modifica la configuracion de las IP tables para que las conexiones vayan a parar al Pod directamente sin pasar por el proceso kube-proxy , por lo que ya no es un proxy estrictamente hablando, pero el nombre se le ha quedado. La diferencia principal entre los dos modos es que en el antiguo al tener que pasar todos los mensajes por el kube-proxy todos tenian que pasar por el espacio de usuario, lo que era una putada para el performance. Ademas el antiguo hac\u00eda un round robin entre los pods de un servicio mientras que el nuevo lo hace de manera aleatoria. Kubernetes Add-ons \u00b6 Hay algunos otros componentes que, sin ser extrictamente necesarios, permiten realizar acciones interesantes en el cluster. Son los add-ons. Estos componentes se instalan en el sistema como Pods, enviando sus manifiestos al API server como cualquier otro pod. Algunos de ellos se instalan mediante Deployment u otro tipo de recurso de k8s title: Algunos add-ons Podemos ver como en Minikube el Ingress Controller y el Dashboard son instalados mediante un [[Kubernetes ReplicationController|ReplicationController]] ```bash $ kubectl get rc -n kube-system NAME DESIRED CURRENT READY AGE default-http-backend 1 1 1 6d kubernetes-dashboard 1 1 1 6d nginx-ingress-controller 1 1 1 6d ``` Mientras que el DNS es un [Deployment](../../../Kubernetes%20Deployment \"Deployment\") ```bash $ kubectl get deploy -n kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-dns 1 1 1 1 6d Como funciona el DNS server \u00b6 Todos los pods en un cluster estan configurados para utilizar el DNS interno por defecto. El DNS se expone mediante un servicio kube-dns para poder asi mover los pods dentro del cluster sin problema. La direccion IP del servicio esta configurada como el nameserver en el fichero /etc/resolv.conf dentro de cada container instalado en el cluster. kube-dns utiliza el mecanismo de notificaciones del API server para observar cambios en los Kubernetes Service y en los Endpoints y actualiza sus tablas internas con cada uno de los cambios. Durante el momento en el que se actualiza un Kubernetes Service o un Kubernetes Endpoint hasta que el kube-dns recibe la notificacion y actualiza sus registros puede que haya informaci\u00f3n erronea en el DNS Ejemplo de como funciona todo junto \u00b6 Todo kubernetes esta formado por peque\u00f1os elementos muy poco acoplados con una buena separacion de responsabilidades ( cada uno hace una cosa y solo una puta cosa) y esa es la raz\u00f3n por la que todo funciona bien Vamos a ver como funcionan todas las piezas en armon\u00eda para la creacion de un Pod. Normalmente no se crea un Pod desde cero, de modo que vamos a ver como se realiza la creacion de un Pod, pero partiendo del manifiesto de un Deployment. Antes siquiera de empezar ya tenemos la siguiente figura, donde cada uno de los componentes est\u00e1 escuchando las notificaciones adecuadas del API server respecto a los recursos que le son interesantes ( normalmente solo uno o un par a lo sumo) Bien, una vez que kubectl en via mediante un HTTP POST el manifiesto del deployment al API Server esta es la cadena de acontecimientos que se desata. Se pueden ver todos los eventos que van sucediendo en el cluster mediante el siguiente comando de kubectl kubectl get events --watch Entendiendo que pasa cuando se ejecuta un POD \u00b6 Cuando ejecutamos un Pod en uno de los nodos no solo se ejecutan los containers que tenemos definidos en el pod, si no que ademas se ejecuta otro pod. Supongamos que solo tenemos un container en el pod para hacerlo mas sencillo. $ kubectl run nginx --image = nginx deployment \"nginx\" created Si vamos al nodo en el que se est\u00e1 ejecutando el pod podemos ver los siguientes contenedores ejecutandose. docker@minikubeVM:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED c917a6f3c3f7 nginx \"nginx -g 'daemon off\" 4 seconds ago 98b8bf797174 gcr.io/.../pause:3.0 \"/pause\" 7 seconds ago Podemos ver que hay un contenedor adicional ejecutandose Dicho contenedor no est\u00e1 ejecutando nada porque esta en pause Ademas podemos ver que se ha creado antes que el contenedor que teniamos definido en el pod Ese contenedor es parte de la infraestructura Todos los contenedores de un mismo pod comparten algunos linux namespaces como por ejemplo el espacio de red \u2026 Este contenedor es el que contiene todos los linux namespaces que son compartidos por todos los contenedores de un mismo pod. Todos los contenedores del mismo pod tienen que compartir los mismos linux namespaces aunque se reinicien, pero eso es posible porque estos est\u00e1n en este container, de manera que aunque los dem\u00e1s se reinicien los namespaces se mantienen. El ciclo de vida de este contenedor esta ligado intimamente al del pod. Si por cualquier motivo este container resultase parado, kubelet volver\u00eda a recrearlo y despu\u00e9s recrear\u00eda todos los demas containers del pod para poder asignarles los linux namespaces del nuevo container. Interpod networking \u00b6 Cada pod tiene un IP unica Todos los pods pueden comunicarse entre si mediante conexiones \u201cplanas\u201d es decir sin NAT Kubernetes no es quien configura esto, es el administrador quien tiene que conseguirlo. Kubernetes solo pone el requisito. La red tiene que estar configurada de tal manera que la IP que un Pod ve de si mismo sea la misma que los dem\u00e1s pueden ver de \u00e9l. Todos los containers deben de poder comunicarse entre ellos independientemente de si estan siendo ejecutados en el mismo o en distintos nodos.","title":"Arquitectura de Kubernetes"},{"location":"pages/kubernetes/architecture/k8s/#arquitectura-de-kubernetes","text":"","title":"Arquitectura de Kubernetes"},{"location":"pages/kubernetes/architecture/k8s/#kubernetes","text":"","title":"kubernetes"},{"location":"pages/kubernetes/architecture/k8s/#componentes","text":"","title":"Componentes"},{"location":"pages/kubernetes/architecture/k8s/#control-plane","text":"Aqui es donde est\u00e1n los elementos que manejan el comportamiento del cluster pero que no ejecutan nada de las aplicaciones de los containers.","title":"Control Plane"},{"location":"pages/kubernetes/architecture/k8s/#componentes_1","text":"etcd API Server #api-server Scheduler Controller Manager","title":"Componentes"},{"location":"pages/kubernetes/architecture/k8s/#the-nodes-workers","text":"Aqui es donde se ejecutan los containers de la aplicaci\u00f3n.","title":"The Nodes ( workers )"},{"location":"pages/kubernetes/architecture/k8s/#componentes_2","text":"Kubelet Kubernetes Service Proxy (kube-proxy) Conainer Runtime (Docker, rkt \u2026 )","title":"Componentes"},{"location":"pages/kubernetes/architecture/k8s/#add-on-components","text":"A parte de los componentes del Control Plane y de los nodos se necesitan unas cuantas cosas para que todo funcione bien Kubernetes DNS Server Dashboard Ingress Controller Heapster Container Network Interface network plugin","title":"Add-on Components"},{"location":"pages/kubernetes/architecture/k8s/#como-se-comunican","text":"Todos se comunican con el API server. El API server es el \u00fanico que se comunica con la base de datos. Todos modifican el estado de la base de datos hablando con el API server. La comunicaci\u00f3n es siempre comenzada por los otros elementos salvo en algunos casos especiales donde el API server es quien comienza la conexion. Cuando usamos kubectl para obtener los logs Cuando usamos kubectl attach Cuando usamos kubectl port-forward","title":"Como se comunican"},{"location":"pages/kubernetes/architecture/k8s/#ejecutando-multiples-instancias","text":"Todos los elementos que tienen los Worker Nodes tienen que correr en el mismo worker node. ( supongo que en cada nodo tienen que estar todos los elementos) Sin embargo los elementos del control plane pueden ser ejecutados en distintos servidores. Se pueden ejecutar multiples instancias de los elementos del Control plane para tener HA. Sin embargo: Etcd y API server pueden tener varias instancias funcionando en paralelo Scheduler y Controller Manager solo pueden tener una instancia funcionando al mismo tiempo mientras que las dem\u00e1s est\u00e1n en stand-by","title":"Ejecutando multiples instancias."},{"location":"pages/kubernetes/architecture/k8s/#como-se-ejecutan-los-componentes","text":"Todos los componentes del control plane y el kube-proxy pueden ejecutarse en el sistema directamente o pueden ejecutarse como pods en el master node . El unico que tiene que ejecutarse como un componente de sistema siempre es Kubelet","title":"Como se ejecutan los componentes"},{"location":"pages/kubernetes/architecture/k8s/#como-se-usan-los-componentes","text":"","title":"Como se usan los componentes"},{"location":"pages/kubernetes/architecture/k8s/#como-utiliza-k8s-etcd","text":"Es el \u00fanico sitio donde se guardan los manifiestos de todos los componentes que se crean en el cluster. Es una base de datos clave-valor distribuida, lo que permite poder tener multiples instancias para tener HA El unico que se comunica con el etcd es el API Server. Esto tiene unas ventajas Se puede cambiar la base de datos con facilidad adaptando el API server. Puede implementarse un sistema de locking optimista Se puede hacer validacion de los elementos antes de guardarlos o de recuperarlos.","title":"Como utiliza K8s ETCD"},{"location":"pages/kubernetes/architecture/k8s/#como-se-guardan-las-cosas-en-etcd","text":"Hay dos versiones en uso v2 y v3 ( aunque seguramente ahora la v2 ya no se use porque la v3 tiene mejor performance.) En v2 Las claves son jerarquicas y estan guardadas en arbol, por lo que puede pensarse en la clave de una entrada como en el nombre completo de un fichero en un arbol de directorios. Por tanto cada clave es: O bien un directorio que a su vez contiene otras claves O bien una clave regular que contiene un valor. En v3 No soporta el concepto de directorios per se, pero como las claves siguen siendo organizadas de manera jer\u00e1rquica y se pueden poner / , podemos seguir pensando en ellas como el nombre completo de un fichero dentro de un filesystem. Todos los datos que kubernetes guarda en etcd se guardan debajo de /registry title: Listado de entradas en etcd V2 ```bash $ etcdctl ls /registry ``` V3: Las entradas cuya clave empieza por un prefijo determinado. ```bash $ etcdtl get /registry --prefix=true ``` Salida ```bash /registry/configmaps /registry/daemonsets /registry/deployments /registry/events /registry/namespaces /registry/pods ... ``` title : pods en el namespace default collapse: true ```bash $ etcdctl ls /registry/pods/default /registry/pods/default/kubia-159041347-xk0vc /registry/pods/default/kubia-159041347-wt6ga /registry/pods/default/kubia-159041347-hp2o5 ``` y esto es lo que se guarda de una entrada en particular ```bash $ etcdctl get /registry/pods/default/kubia-159041347-wt6ga {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\", \"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":... ``` Como puede verse es un documento en formato json.","title":"Como se guardan las cosas en etcd"},{"location":"pages/kubernetes/architecture/k8s/#consistencia-en-etcd-cuando-hay-un-cluster","text":"Como podemos tener multiples instancias de etcd en un cluster tenemos que asegurar que hay consistencia en el estado que muestran todas. Para esto se utiliza el [[Algoritmo RAFT]] Es un algoritmo de consenso, de manera que para transicionar a un nuevo estado, mas de la mitad de los nodos tienen que estar de acuerdo. En este algoritmo se asegura que una determinada instancia de etcd o bien muestra el ultimo estado al que se ha llegado o bien muestra uno de los antiguos estados a los que se llego por consenso. Si se produce un split brain, si una de las particiones tiene la mayoria de los nodos evidentemente la otra no los tendr\u00e1. Por tanto la primera particion si que podr\u00e1 transicionar de estado porque tienen la mayoria, mientras que la otra no podr\u00e1 modificar el estado del cluster, qued\u00e1ndose en un estado v\u00e1lido., Cuando la situacion se solucione los nodos que estaban en un estado antiguo, pero valido, podr\u00e1n recuperarse y hacer catch-up. Debido a la necesidad de quorum es mejor siempre tener un numero impar de instancias de etcd. Teniendo un numero par no aumentamos el numero de fallos de instancias a los que somos resilientes, pero si que aumentamos la posibilidad de fallo, ya que a mas instancias en el cluster mas posibilidades de que alguna falle. Normalmente los clusters mas grandes de etcd tienen CINCO o SIETE instancias, lo que permite manejar sin problemas el fallo de DOS o TRES instancias, lo que suele ser suficiente en la mayoria de los casos.","title":"Consistencia en ETCD cuando hay un cluster."},{"location":"pages/kubernetes/architecture/k8s/#que-hace-el-api-server","text":"","title":"Que hace el API server"},{"location":"pages/kubernetes/architecture/k8s/#intro","text":"Es el componente que todos los demas componentes, incluido el kubectl utilizan para poder interactuar con el sistema. Provee un interface CRUD sobre un API Rest Guarda todos los datos en ETCD Provee consistencia en los datos que se guardan Chequea los datos que los demas componentes quieren guardar en etcd antes de guardarlos para evitar estados inconsistentes del sistema Implementa un Optimistic Locking system Dado que podemos tener multiples instancias de API server ejecutandose de manera concurrente tenemos que evitar que los cambios puedan sobreescribir otros cambios que se est\u00e9n haciendo en ese momento 'kubectl' es uno de los clientes del API server. Cuando creamos un nuevo recurso con un fichero `yaml` lo que hace es enviar un HTTP POST con el fichero en formato json y esto es lo que pasa en el API Server ![[APIServerHTTPPOST.png]]","title":"Intro"},{"location":"pages/kubernetes/architecture/k8s/#stages","text":"","title":"Stages"},{"location":"pages/kubernetes/architecture/k8s/#authentication","text":"Lo primero que se hace es autenticar al cliente que envia la peticion. Se envia la peticion a una lista de plugins de autenticacion que esten configurados en el API server, pasando por todos hasta que uno de ellos sea capaz de determinar quien es el cliente que envia la peticion.","title":"Authentication"},{"location":"pages/kubernetes/architecture/k8s/#authorization","text":"Una vez que sabemos quien es el cliente que pide la accion y que podemos confiar en el, tenemos que ver si dicho cliente tiene autorizacion para poder hacer dicha accion sobre el recurso que esta pidiendo hacerla. Para esto estan los plugins de autorizacion. La peticion va pasando por los plugins hasta que uno de ellos dice que el cliente puede realizar al accion sobre el recurso pedido.","title":"Authorization"},{"location":"pages/kubernetes/architecture/k8s/#admision-control","text":"Las operaciones de Creacion, Modificacion y Borrado de recursos pasan por una fase adicional, la de Control de Admision. En esta fase la peticion pasas por todos los plugins . Estos plugins pueden modificar la peticion A\u00f1adiendo valores por defecto que no existen en el recurso de la peticion Cambiando valores del recursos que exceden limites Incluso pueden modificar recursos relacionados que no est\u00e9n en la peticion Tambien pueden validad y rechazar la peticion por el contenido que tiene. title: Ejemplos de Admission Control Plugins. - `AlwaysPullImages` -> Sobreescribe el `imagePullPolicy` de los Pods y lo pone a `Always` - `ServiceAccount` -> Aplica un `ServiceAccount` por defecto a todos los Pods que no definen uno. - `NamespaceLifecycle` -> Evita que se puedan crear Pods en namespaces que est\u00e1n en proceso de borrado o que no existen. - `ResourceQuota` -> Se asegura que los Pods de un determinado namespace solo puedan utilizar unas cantidades de CPU y memoria determinadas","title":"Admision Control"},{"location":"pages/kubernetes/architecture/k8s/#how-api-server-notifies-clients","text":"El API server permite que los demas componentes pidan ser notificados cuando un recurso es Creado Modificado o Borrado, de manera que pueda hacer lo que sea que tenga que hacer en respuesta a esta operacion. El Cliente abre una conexion HTTP al API server y a trav\u00e9s de dicha conexion el cliente recive un stream de moficiaciones. Cada vez que uno de los objetos observados cambia se le envia al cliente la nueva version del objeto. Este mecanismo lo usa por ejemplo el kubectl cuando pedimos que nos haga un watch de los pods kubectl get pods -o yaml --watch Tambien lo utiliza el Scheduler","title":"How API Server Notifies clients."},{"location":"pages/kubernetes/architecture/k8s/#entendiendo-el-funcionamiento-del-scheduler","text":"","title":"Entendiendo el funcionamiento del Scheduler"},{"location":"pages/kubernetes/architecture/k8s/#controllers-ejecutandose-en-el-controller-manager","text":"","title":"Controllers ejecutandose en el Controller Manager."},{"location":"pages/kubernetes/architecture/k8s/#intro_1","text":"Hasta ahora tenemos en el control plane: ETCD: que solo guarda los manifiestos. API Server: que es un interface para que los demas puedan guardar cosas en la base de datos y ser notificados Scheduler: Que se encarga de asingar nodos a los nuevos pods. Pero no tenemos nadie que haga nada para mantener el estado del cluster conciliado con el estado de los manifiestos guardados en la base de datos. Esto es lo que hace el Controller Manager, o para ser mas exactlos la lista de controllers que se ejecutan en el. Todos los controller se al formar parte del Control Plane, se ejecutan en el Master Node","title":"Intro"},{"location":"pages/kubernetes/architecture/k8s/#controllers-en-general","text":"Actualmente ( no tengo claro que eso sea cierto ahora mismo) un solo proceso Controller Manager combina dentro de el varios controllers distintos que realizan distintas labores. Eventualmente cada uno de estos controllers ser\u00e1 separado en un proceso distinto para que sea mas sencillo cambiarlos. Lista de Controllers: Replication Manager \u2013 Un controller para ReplicationController resources ReplicaSet Controller DaemonSet Controller Job Controller Deployment Controller StatefulSet Controller Node Controller Service Controller Endpoints Controller Namespace Controller PersistentVolumeController Others \u2026 que cachondo. Basicamente hay un Controller por cada uno de los recursos de kubernetes. Los recursos son una declaracion de cual queremos que sea el estado de un cluster. Los Controllers realizan el trabajo de intentar conciliar el estado actual del cluster con el que queremos que sea. En general los controllers tienen un \u201creconciliation loop\u201d donde intentan hacer esta reconciliacion del estado creando, modificando o borrando determinados recursos. collapse:true En cada recurso el `spec` es donde ponemos el estado esperado mientras que los controllers escriben en el `status` el estado actual. Utilizan el sistema de notificacion del API server para saber cuando se modifican los recursos a los que atienden. Tambien tienen una operacion \u201cre-list\u201d que ejecutan periodicamente por si acaso han perdido alguna notificacion. Los Controllers no hablan nunca unos con otros. Los Controllers no saben ni que existen otros controllers. Solo hablan con el API server nada mas. Cada uno hace su parte y esperan que todo funcione correctamente.","title":"Controllers en general"},{"location":"pages/kubernetes/architecture/k8s/#algunos-controllers","text":"","title":"Algunos controllers"},{"location":"pages/kubernetes/architecture/k8s/#replication-manager","text":"Basicamente espera ser notificado de los Pods y los ReplicationController que se van creando, destruyendo o modificando para comprobar si lo que se ha puesto en los ReplicationController se sigue cumpliendo. En el caso de que el n\u00famero de pods no sea el esperado realiza las operaciones adecuadas para poder llegar al estado deseado. Si necesita crear nuevos pods utiliza el spec.template del ReplicationController para enviar un HTTP POST al API Server con el nuevo manifiesto para que se desencadene la creacion de un nuevo pod. El Replication Manager hace su trabajo solo modificando los manifiestos de los recursos mediante el API server como hacen todos los demas controllers","title":"Replication Manager"},{"location":"pages/kubernetes/architecture/k8s/#replicaset-controller-daemonset-controller-y-job-controller","text":"Los tres hacen basicamente lo mismo que el Replication Manager, solo que usando distintos recursos\u2026 Kubernetes ReplicaSet , Kubernetes DaemonSet y Kubernetes Job","title":"ReplicaSet Controller, DaemonSet Controller y Job Controller"},{"location":"pages/kubernetes/architecture/k8s/#deployment-controller","text":"Gestiona las actualizaciones de los Deployment . Hace un roll-out del deployment cuando se modifica. Basicamente crea un nuevo Replicaset y despues realiza una escalacion tanto del antiguo como del nuevo Replicaset basandose en la estrategia definida en el Deployment hasta que todos los viejos Pods han desaparecido y han sido reemplazados por los nuevos.","title":"Deployment Controller"},{"location":"pages/kubernetes/architecture/k8s/#statefulset-controller","text":"Como los demas Controllers el StatefulSet Controller crea y destruye pods de acuerdo con la especificacion de los StatefulSet . Adem\u00e1s es el \u00fanico que tambien maneja los PersistentVolumeClaims para cada instancia de Pod","title":"StatefulSet Controller"},{"location":"pages/kubernetes/architecture/k8s/#node-controller","text":"Maneja los Node Resources que describen los workers de un cluster. Entre otras cosas mantiene una lista actualizada de los nodos del cluster monitorizando las maquinas que se a\u00f1aden o quitan. Tambien monitoriza la salud de los nodos y elimina los Pods de los nodos que no pueden ser alcanzados.","title":"Node Controller"},{"location":"pages/kubernetes/architecture/k8s/#service-controller","text":"Cuando un Kubernetes Service de tipo LoadBalancer se crea se pide un LoadBalancer al sistema para poder hacer el Servicio visible desde el exterior. Este controler es el encargado de realizar la operacion de pedir el recurso y de liberarlo cuando no es necesario.","title":"Service Controller"},{"location":"pages/kubernetes/architecture/k8s/#endpoints-controller","text":"Los Kubernetes Service no son enlazados directamente a los Pods, si no que contienen una lista de los endpotins (IP + Puerto), lista que es actualizada de acuerdo con el pod selector del Kubernetes Service El Endpoints Controller es el encargado de mantener la lista de endpoints constantemente actualizada con las IPs y puertos de los Pods que matchean el label selector","title":"Endpoints Controller"},{"location":"pages/kubernetes/architecture/k8s/#namespace-controller","text":"Cuando un namespace es borrado todos los recursos que pertenezcan a dicho namespace deben ser borrados. Esto es lo que hace el controller.","title":"Namespace Controller"},{"location":"pages/kubernetes/architecture/k8s/#persistentvolume-controller","text":"Cuando un usuario crea un Kubernetes PersistentVolumeClaims kubernetes tiene que encontrar el Kubernetes PersistentVolume al que poder enlazarlo. Esto es justamente lo que hace este controller. Cuando se crea un nuevo Kubernetes PersistentVolumeClaims este controller intenta encontrar el mejor match para el. Busca el Kubernetes PersistentVolume mas peque\u00f1o que tenga el modo de acceso adecuado y que tenga una capacidad suficiente para el claim. Para esto mantiene una lista ordenada de Kubernetes PersistentVolume para cada uno de los modos de acceso por orden ascendente de capacidad ( asi que solo tiene que devolver el primero de la lista.) Cuando el Kubernetes PersistentVolumeClaims es borrado el Kubernetes PersistentVolume es reclamado por el controller de acuerdo con la politica de reclaim \u2026 ( puede quedarse como est\u00e1, ser borrado, ser vaciado\u2026)","title":"PersistentVolume Controller"},{"location":"pages/kubernetes/architecture/k8s/#kubelet","text":"","title":"Kubelet"},{"location":"pages/kubernetes/architecture/k8s/#intro_2","text":"Al contrario que los controllers tango kubelet como kube-proxy se ejecutan en los worker nodes, que es donde se ejecutan los containers de las aplicaciones.","title":"Intro"},{"location":"pages/kubernetes/architecture/k8s/#responsabilidades","text":"Kubelet es el responsable de todo lo que sucede en un worker node. Lo primero que tiene que hacer es registrar el nodo en el cluster, para eso crea un Kubernetes Node Resources usando el API Server. A partir de ese momento monitoriza todos los Pods para ver cuales son asignados por el Scheduler para ser ejecutados en su nodo, y cuando esto sucede habla con el Container Runtime que haya configurado para que se ejecuten los containers del Pod. Tambi\u00e9n monitoriza constantemente los containers de los Pods que est\u00e1n ejecutandose en su Nodo para poder reportar su estatus, eventos y consumo de recursos al API Server. Kubelet tambien es el responsable de ejecutar los liveness probes, rearrancar los containers cuando la probe falle y por \u00faltimo ser quien borre los contenedores cuando el Pod sea borrado.","title":"Responsabilidades"},{"location":"pages/kubernetes/architecture/k8s/#ejecutando-pods-staticos-sin-el-api-server","text":"Normalmente kubelet obtiene los manifiestos de los pods que tiene que ejecutar del API server, pero tambien puede ejecutar otros Pods obteniendo los manifiestos de un directorio del Nodo en el que se ejecuta. Esto se utiliza principalmente para poder ejecutar componentes del Control Plane como Pods en lugar de como procesos del master node.","title":"Ejecutando Pods staticos sin el API Server."},{"location":"pages/kubernetes/architecture/k8s/#kubernetes-service-proxy","text":"Los Worker Nodes tambien ejecutan el kube-proxy. El \u00fanico cometido de kube-proxy es asegurarse que cuando llamamos a uno de los Kubernetes Service la llamada termina en uno de los Pods que est\u00e1 dando soporte a dicho servicio. Cuando hay mas de un Pod dando soporte a dicho servicio se realiza un load-balancing. Se llama kube-proxy porque en la primera de las implementaciones era un proceso en espacio de usuario que hab\u00eda modificado las IP tables para que las peticiones a los servicios fueran redirigidas a \u00e9l y luego fuese el quien las reenviase a los Pods adecuados. Todas las conexiones pasaban por \u00e9l. Ahora hay una implementaci\u00f3n mucho mejor en performance que simplemente modifica la configuracion de las IP tables para que las conexiones vayan a parar al Pod directamente sin pasar por el proceso kube-proxy , por lo que ya no es un proxy estrictamente hablando, pero el nombre se le ha quedado. La diferencia principal entre los dos modos es que en el antiguo al tener que pasar todos los mensajes por el kube-proxy todos tenian que pasar por el espacio de usuario, lo que era una putada para el performance. Ademas el antiguo hac\u00eda un round robin entre los pods de un servicio mientras que el nuevo lo hace de manera aleatoria.","title":"Kubernetes Service Proxy"},{"location":"pages/kubernetes/architecture/k8s/#kubernetes-add-ons","text":"Hay algunos otros componentes que, sin ser extrictamente necesarios, permiten realizar acciones interesantes en el cluster. Son los add-ons. Estos componentes se instalan en el sistema como Pods, enviando sus manifiestos al API server como cualquier otro pod. Algunos de ellos se instalan mediante Deployment u otro tipo de recurso de k8s title: Algunos add-ons Podemos ver como en Minikube el Ingress Controller y el Dashboard son instalados mediante un [[Kubernetes ReplicationController|ReplicationController]] ```bash $ kubectl get rc -n kube-system NAME DESIRED CURRENT READY AGE default-http-backend 1 1 1 6d kubernetes-dashboard 1 1 1 6d nginx-ingress-controller 1 1 1 6d ``` Mientras que el DNS es un [Deployment](../../../Kubernetes%20Deployment \"Deployment\") ```bash $ kubectl get deploy -n kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-dns 1 1 1 1 6d","title":"Kubernetes Add-ons"},{"location":"pages/kubernetes/architecture/k8s/#como-funciona-el-dns-server","text":"Todos los pods en un cluster estan configurados para utilizar el DNS interno por defecto. El DNS se expone mediante un servicio kube-dns para poder asi mover los pods dentro del cluster sin problema. La direccion IP del servicio esta configurada como el nameserver en el fichero /etc/resolv.conf dentro de cada container instalado en el cluster. kube-dns utiliza el mecanismo de notificaciones del API server para observar cambios en los Kubernetes Service y en los Endpoints y actualiza sus tablas internas con cada uno de los cambios. Durante el momento en el que se actualiza un Kubernetes Service o un Kubernetes Endpoint hasta que el kube-dns recibe la notificacion y actualiza sus registros puede que haya informaci\u00f3n erronea en el DNS","title":"Como funciona el DNS server"},{"location":"pages/kubernetes/architecture/k8s/#ejemplo-de-como-funciona-todo-junto","text":"Todo kubernetes esta formado por peque\u00f1os elementos muy poco acoplados con una buena separacion de responsabilidades ( cada uno hace una cosa y solo una puta cosa) y esa es la raz\u00f3n por la que todo funciona bien Vamos a ver como funcionan todas las piezas en armon\u00eda para la creacion de un Pod. Normalmente no se crea un Pod desde cero, de modo que vamos a ver como se realiza la creacion de un Pod, pero partiendo del manifiesto de un Deployment. Antes siquiera de empezar ya tenemos la siguiente figura, donde cada uno de los componentes est\u00e1 escuchando las notificaciones adecuadas del API server respecto a los recursos que le son interesantes ( normalmente solo uno o un par a lo sumo) Bien, una vez que kubectl en via mediante un HTTP POST el manifiesto del deployment al API Server esta es la cadena de acontecimientos que se desata. Se pueden ver todos los eventos que van sucediendo en el cluster mediante el siguiente comando de kubectl kubectl get events --watch","title":"Ejemplo de como funciona todo junto"},{"location":"pages/kubernetes/architecture/k8s/#entendiendo-que-pasa-cuando-se-ejecuta-un-pod","text":"Cuando ejecutamos un Pod en uno de los nodos no solo se ejecutan los containers que tenemos definidos en el pod, si no que ademas se ejecuta otro pod. Supongamos que solo tenemos un container en el pod para hacerlo mas sencillo. $ kubectl run nginx --image = nginx deployment \"nginx\" created Si vamos al nodo en el que se est\u00e1 ejecutando el pod podemos ver los siguientes contenedores ejecutandose. docker@minikubeVM:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED c917a6f3c3f7 nginx \"nginx -g 'daemon off\" 4 seconds ago 98b8bf797174 gcr.io/.../pause:3.0 \"/pause\" 7 seconds ago Podemos ver que hay un contenedor adicional ejecutandose Dicho contenedor no est\u00e1 ejecutando nada porque esta en pause Ademas podemos ver que se ha creado antes que el contenedor que teniamos definido en el pod Ese contenedor es parte de la infraestructura Todos los contenedores de un mismo pod comparten algunos linux namespaces como por ejemplo el espacio de red \u2026 Este contenedor es el que contiene todos los linux namespaces que son compartidos por todos los contenedores de un mismo pod. Todos los contenedores del mismo pod tienen que compartir los mismos linux namespaces aunque se reinicien, pero eso es posible porque estos est\u00e1n en este container, de manera que aunque los dem\u00e1s se reinicien los namespaces se mantienen. El ciclo de vida de este contenedor esta ligado intimamente al del pod. Si por cualquier motivo este container resultase parado, kubelet volver\u00eda a recrearlo y despu\u00e9s recrear\u00eda todos los demas containers del pod para poder asignarles los linux namespaces del nuevo container.","title":"Entendiendo que pasa cuando se ejecuta un POD"},{"location":"pages/kubernetes/architecture/k8s/#interpod-networking","text":"Cada pod tiene un IP unica Todos los pods pueden comunicarse entre si mediante conexiones \u201cplanas\u201d es decir sin NAT Kubernetes no es quien configura esto, es el administrador quien tiene que conseguirlo. Kubernetes solo pone el requisito. La red tiene que estar configurada de tal manera que la IP que un Pod ve de si mismo sea la misma que los dem\u00e1s pueden ver de \u00e9l. Todos los containers deben de poder comunicarse entre ellos independientemente de si estan siendo ejecutados en el mismo o en distintos nodos.","title":"Interpod networking"},{"location":"pages/kubernetes/scheduler/Kubernetes%20scheduler%20Pod%20Priority%20and%20Preemption/","tags":["kubernetes"],"text":"Pod Priority and Preemption \u00b6 kubernetes scheduling preemption \u00b6 Links: main Info \u00b6 FEATURE STATE: \u2003 Kubernetes v1.14 [stable] Los Pod tienen una prioridad. Esta prioridad indica la importancia relativa que tiene un pod con respecto a los dem\u00e1s Si un Pod no puede ser asignado a un nodo el scheduler intenta eliminar (evict) pods que tengan menos prioridad para poder asignar el pod. How to use it. \u00b6 Crear una o mas Kubernetes PriorityClass Crear pods con el campo priorityClassName seteado a alguno de los nombres de alguna de las PriorityClass que hayamos definido. Workflow \u00b6 Cuando un Pod con PriorityClass entra el priority admission controller busca la clase y popla el campo prioridad con el valor entero. Cuando las prioridades est\u00e1n activadas, la cola de pods esperando scheduling se ordena por prioridad para que los pods con mayor prioridad sean asignados antes. Como existe el Kubernetes scheduler back-off todavia es posible que un nodo con menor prioridad pero con menos requisitos de recursos, sea asignado a un nodo antes que otro pod de mayor prioridad pero con unos requisitos de recursos mayores que no se pueden cumplir. Preemption \u00b6 more info Los pods pendientes de scheduling se meten en una cola a esperar. El scheduler toma un pod de la cola e intenta asignarlo a un Kubernetes Node Si ning\u00fan nodo satisface los requisitos del Pod se lanza la logica de preemption para dicho pod Definicion: Logica de Preemption Supongamos que el Pod es P . La l\u00f3gica de preempcion intenta en contrar un Nodo N tal que la eliminaci\u00f3n de uno o mas Pods de menor prioridad que P permita que se satisfagan los requisitos de P y pueda ser asignado al nodo N Si se encuentra un nodo que cumpla esto los pods de menor prioridad son sacados del nodo para que mas tarde P sea asignado al nodo. Cuando se encuentra un nodo N donde eliminando pods de menor prioridad podemos meter a P el nombre del nodo se mete en un campo del pod llamado nominatedNodeName . Esto es as\u00ed porque el pod P no es asignado a N directamente, si no que se lanza la operacion de eliminacion de los pods de N para hacer sitio a P y esta operaci\u00f3n puede tardar un tiempo ( porque los pods pueden tener tiempos de graceful termination por ejemplo), asi que lo que se hace es retrasar la operacion de asignacion de P , pero cuando se retoma se tiene en cuenta el valor del campo nominatedNodeName como nodo preferente P no tiene porque ser asignado necesariamente al nodo en nominatedNodeName El scheduler siempre intenta primero el nominatedNodeName antes de iterar por los demas nodos en busca de alguno pero, como la operacion de asignacion de P no se sincroniza con la de eliminaci\u00f3n de los otros pods de N , puede pasar que entre medias haya otro nodo que se libere de recursos y donde podamos asignar a P Es decir nominatedNodeName y nodeName no tienen porque tener el mismo valor. Tambien puede pasar que, una vez liberado el espacio de N llegue un pod P1 con una prioridad mayor que la de P y el scheduler le asigne antes al nodo N ocupando los recursos que en principio se liberaron para P . En este caso el scheduler borra el valor del campo nominatedNodeName de P , esto hace que P pueda ser elegible para lanzar la eliminacion de pods en otro nodo Kubernetes Preemption Evaluator and Interface","title":"Kubernetes Pod Priority and Preemption"},{"location":"pages/kubernetes/scheduler/Kubernetes%20scheduler%20Pod%20Priority%20and%20Preemption/#pod-priority-and-preemption","text":"","title":"Pod Priority and Preemption"},{"location":"pages/kubernetes/scheduler/Kubernetes%20scheduler%20Pod%20Priority%20and%20Preemption/#kubernetes-scheduling-preemption","text":"Links: main","title":"kubernetes scheduling preemption"},{"location":"pages/kubernetes/scheduler/Kubernetes%20scheduler%20Pod%20Priority%20and%20Preemption/#info","text":"FEATURE STATE: \u2003 Kubernetes v1.14 [stable] Los Pod tienen una prioridad. Esta prioridad indica la importancia relativa que tiene un pod con respecto a los dem\u00e1s Si un Pod no puede ser asignado a un nodo el scheduler intenta eliminar (evict) pods que tengan menos prioridad para poder asignar el pod.","title":"Info"},{"location":"pages/kubernetes/scheduler/Kubernetes%20scheduler%20Pod%20Priority%20and%20Preemption/#how-to-use-it","text":"Crear una o mas Kubernetes PriorityClass Crear pods con el campo priorityClassName seteado a alguno de los nombres de alguna de las PriorityClass que hayamos definido.","title":"How to use it."},{"location":"pages/kubernetes/scheduler/Kubernetes%20scheduler%20Pod%20Priority%20and%20Preemption/#workflow","text":"Cuando un Pod con PriorityClass entra el priority admission controller busca la clase y popla el campo prioridad con el valor entero. Cuando las prioridades est\u00e1n activadas, la cola de pods esperando scheduling se ordena por prioridad para que los pods con mayor prioridad sean asignados antes. Como existe el Kubernetes scheduler back-off todavia es posible que un nodo con menor prioridad pero con menos requisitos de recursos, sea asignado a un nodo antes que otro pod de mayor prioridad pero con unos requisitos de recursos mayores que no se pueden cumplir.","title":"Workflow"},{"location":"pages/kubernetes/scheduler/Kubernetes%20scheduler%20Pod%20Priority%20and%20Preemption/#preemption","text":"more info Los pods pendientes de scheduling se meten en una cola a esperar. El scheduler toma un pod de la cola e intenta asignarlo a un Kubernetes Node Si ning\u00fan nodo satisface los requisitos del Pod se lanza la logica de preemption para dicho pod Definicion: Logica de Preemption Supongamos que el Pod es P . La l\u00f3gica de preempcion intenta en contrar un Nodo N tal que la eliminaci\u00f3n de uno o mas Pods de menor prioridad que P permita que se satisfagan los requisitos de P y pueda ser asignado al nodo N Si se encuentra un nodo que cumpla esto los pods de menor prioridad son sacados del nodo para que mas tarde P sea asignado al nodo. Cuando se encuentra un nodo N donde eliminando pods de menor prioridad podemos meter a P el nombre del nodo se mete en un campo del pod llamado nominatedNodeName . Esto es as\u00ed porque el pod P no es asignado a N directamente, si no que se lanza la operacion de eliminacion de los pods de N para hacer sitio a P y esta operaci\u00f3n puede tardar un tiempo ( porque los pods pueden tener tiempos de graceful termination por ejemplo), asi que lo que se hace es retrasar la operacion de asignacion de P , pero cuando se retoma se tiene en cuenta el valor del campo nominatedNodeName como nodo preferente P no tiene porque ser asignado necesariamente al nodo en nominatedNodeName El scheduler siempre intenta primero el nominatedNodeName antes de iterar por los demas nodos en busca de alguno pero, como la operacion de asignacion de P no se sincroniza con la de eliminaci\u00f3n de los otros pods de N , puede pasar que entre medias haya otro nodo que se libere de recursos y donde podamos asignar a P Es decir nominatedNodeName y nodeName no tienen porque tener el mismo valor. Tambien puede pasar que, una vez liberado el espacio de N llegue un pod P1 con una prioridad mayor que la de P y el scheduler le asigne antes al nodo N ocupando los recursos que en principio se liberaron para P . En este caso el scheduler borra el valor del campo nominatedNodeName de P , esto hace que P pueda ser elegible para lanzar la eliminacion de pods en otro nodo Kubernetes Preemption Evaluator and Interface","title":"Preemption"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/","tags":["kubernetes"],"text":"Kubernetes Scheduling Framework \u00b6 kubernetes scheduling \u00b6 Intro \u00b6 Basicamente a\u00f1ade una manera de extender el Scheduler. Los plugins se compilan con el Scheduler y le proveen de API adicional. De esta manera el core del scheduler no cambia pero se pueden a\u00f1adir nuevas funcionalidades. Workflow \u00b6 El Framework define una serie de puntos donde los plugins pueden registrarse para ser llamados. Los plugins pueden cambiar las decisiones del scheduler o bien simplemente ampliar la informaci\u00f3n disponible. Todo intento de hacer scheduling de un nodo tiene dos fases diferenciadas: Scheduling Cycle Binding Cycle A esto se le llama scheduling context Scheduling Cycle & Binding Cycle. \u00b6 Scheduling Cycle Es el proceso de elegir un nodo a un Pod Se ejecuta en serie ( es decir hasta que no terminamos un pod no empezamos con el siguiente) Es alimentado por una cola ordenada de pods esperando ser asignados a un nodo. Binding Cycle Es donde se aplica la decision anterior al cluster Se pueden ejecutar en paralelo. Extension Points \u00b6 Scheduling Cycle QueueSort PreFilter Filter Post Filter Pre Score Score NormalizeScore Reserve Permit Binding Cycle PreBind Bind PostBind Scheduling Cycle \u00b6 QueueSort \u00b6 Ordenan los Pods en la cola de espera Solo puede haber un plugin de este tipo activo al mismo tiempo PreFilter \u00b6 Comprueban cierta informacion sobre el Pod y condiciones que tanto el pod como el cluster tienen que cumplir. Si uno de los PreFilter Plugins retorna error se aborta el Scheduling Cycle. Filter \u00b6 Se usa para Filtrar los Nodos que NO pueden ejecutar el pod. Para cada nodo se ejecutan todos los plugins en el orden configurado Si un filtro marca un nodo como no v\u00e1lido no se ejecutan los demas en dicho nodo Es decir si ya sabemos que un nodo no es valido no tiene sentido continuar Los nodos pueden ser evaluados de manera concurrente Post Filter \u00b6 Estos plugins se llaman despues de la fase de \u201cFilter\u201d pero solo cuando ningun nodo ha pasado el filtrado ( es decir que no tenemos ning\u00fan nodo v\u00e1lido para poder meter el pod.) Aqui ser\u00eda el momento de eliminar pods de un nodo para hacer sitio al nuevo de ser necesario. \u2013> Kubernetes scheduler Pod Priority and Preemption Se llama a todos los plugins en el orden configurado. Si cualquier plugin marca un nodo como Schedulable (valido) el resto de plugins no se llaman. Supongo que, una vez encuentra uno donde encajar el pod no sigue porque se supone que estas buscando el \u201cmenos malo\u201d Pre Score \u00b6 Sirven para hacer un pre-score de los pods, y luego poder compartir esa informacion con el resto de los plugins de Score. Si uno de estos plugins retorna un error se aborta el Scheduling Cycle Score \u00b6 Se utiliza para dar una puntuacion ( scoring) a todos los nodos que han pasado el filtro. Se llama a cada uno de los plugins de score para cada uno de los nodos. Hay un rango definido de enteros para representar el score de un nodo. Despues de la fase de Normalizacion el scheduler combinar\u00e1 la puntuacion de todos los plugins para un nodo segun el peso que cada plugin tenga configurado NormalizeScore \u00b6 Sirven para adaptar la puntuacion dada por un Score plugin al intervalo definido. Un plugin que se registra para esta extension solo ser\u00e1 llamado con la informaci\u00f3n que el mismo gener\u00f3 en la fase de Score Esto se llama una vez por plugin y por scheduling cycle. Si un plugin de estos retorna un error se aborta el Scheduling cycle. Reserve \u00b6 Los plugins que se registren aqui tienen que tener dos metodos Reserve & Unreserve . Esto se llama antes de que empiece la fase de Binding para evitar condiciones de carrera. Basicamente se les da la oportunidad a los plugins de \u201creservar\u201d ciertos recursos de un nodo para que no sean tenidos en cuenta en los subsiguientes Scheduling cycles aun cuando no se ha producido realmente el binding, para evitar que haya varios Pods luchando por los mismos recursos. El metodo Reserve puede fallar. Si un plugin falla no se llama al resto de los plugins y se considera que la fase ha fallado, abortando el Scheduling cycle. Unreserve se llama si falla la fase de Reserva en un plugin posterior o si falla alguna de las fases posteriores a la de Reserve. Cuando esto pasa se llama al Unreserve de todos los plugins en el orden inverso al de las llamadas a Reverse . La implementacion de los Unreserve tiene que ser idempotente y no puede fallar. Permit \u00b6 Es el ultimo punto de ejecucion en el Scheduling Cycle. Permite denegar un scheduling o retrasarlo. Un permit plugin puede hacer una de las tres siguientes cosas: approve : Se env\u00eda al Binding Cycle inmediatamente. deny : Se cancela el Binding y se envia el pod de nuevo a la cola de espera para Scheduling. wait (with timeout) : Env\u00eda el pod a una cola de waiting . El Binding Cycle del pod comienza pero queda inmediatamente bloqueado hasta ser aprovado Si ning\u00fan plugin lo aprueba antes de que pase el timeout el wait se convierte en deny y el Pod se devuelve a la cola de espera para el Scheduling. Binding Cycle \u00b6 PreBind \u00b6 Aqui se debe hacer cualquier trabajo preliminar antes de permitir que el pod se ejecute en el nodo Ej: Montar un volumen de red Si cualquiera de los plugins retorna un error el pod es rejected y enviado a la cola de Scheduling Bind \u00b6 Se encargan de hacer el bind de un pod a un nodo ( no tengo ni idea de que implica esto ahora mismo.) Se llama a cada plugin en el orden configurado No son llamados hasta que no han terminado los de PreBinding Un plugin puede elegir si maneja o no un determinado Pod. En el caso de que un plugin elija manejarlo no se sigue llamando a los demas para ese pod PostBind \u00b6 Esto es basicamente para informar de que el proceso ha terminado Puede utilizarse para hacer limpieza de recursos utilizados en el proceso. Links \u00b6 official k8s doc","title":"Kubernetes Scheduling Framework"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#kubernetes-scheduling-framework","text":"","title":"Kubernetes Scheduling Framework"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#kubernetes-scheduling","text":"","title":"kubernetes scheduling"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#intro","text":"Basicamente a\u00f1ade una manera de extender el Scheduler. Los plugins se compilan con el Scheduler y le proveen de API adicional. De esta manera el core del scheduler no cambia pero se pueden a\u00f1adir nuevas funcionalidades.","title":"Intro"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#workflow","text":"El Framework define una serie de puntos donde los plugins pueden registrarse para ser llamados. Los plugins pueden cambiar las decisiones del scheduler o bien simplemente ampliar la informaci\u00f3n disponible. Todo intento de hacer scheduling de un nodo tiene dos fases diferenciadas: Scheduling Cycle Binding Cycle A esto se le llama scheduling context","title":"Workflow"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#scheduling-cycle-binding-cycle","text":"Scheduling Cycle Es el proceso de elegir un nodo a un Pod Se ejecuta en serie ( es decir hasta que no terminamos un pod no empezamos con el siguiente) Es alimentado por una cola ordenada de pods esperando ser asignados a un nodo. Binding Cycle Es donde se aplica la decision anterior al cluster Se pueden ejecutar en paralelo.","title":"Scheduling Cycle &amp; Binding Cycle."},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#extension-points","text":"Scheduling Cycle QueueSort PreFilter Filter Post Filter Pre Score Score NormalizeScore Reserve Permit Binding Cycle PreBind Bind PostBind","title":"Extension Points"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#scheduling-cycle","text":"","title":"Scheduling Cycle"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#queuesort","text":"Ordenan los Pods en la cola de espera Solo puede haber un plugin de este tipo activo al mismo tiempo","title":"QueueSort"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#prefilter","text":"Comprueban cierta informacion sobre el Pod y condiciones que tanto el pod como el cluster tienen que cumplir. Si uno de los PreFilter Plugins retorna error se aborta el Scheduling Cycle.","title":"PreFilter"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#filter","text":"Se usa para Filtrar los Nodos que NO pueden ejecutar el pod. Para cada nodo se ejecutan todos los plugins en el orden configurado Si un filtro marca un nodo como no v\u00e1lido no se ejecutan los demas en dicho nodo Es decir si ya sabemos que un nodo no es valido no tiene sentido continuar Los nodos pueden ser evaluados de manera concurrente","title":"Filter"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#post-filter","text":"Estos plugins se llaman despues de la fase de \u201cFilter\u201d pero solo cuando ningun nodo ha pasado el filtrado ( es decir que no tenemos ning\u00fan nodo v\u00e1lido para poder meter el pod.) Aqui ser\u00eda el momento de eliminar pods de un nodo para hacer sitio al nuevo de ser necesario. \u2013> Kubernetes scheduler Pod Priority and Preemption Se llama a todos los plugins en el orden configurado. Si cualquier plugin marca un nodo como Schedulable (valido) el resto de plugins no se llaman. Supongo que, una vez encuentra uno donde encajar el pod no sigue porque se supone que estas buscando el \u201cmenos malo\u201d","title":"Post Filter"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#pre-score","text":"Sirven para hacer un pre-score de los pods, y luego poder compartir esa informacion con el resto de los plugins de Score. Si uno de estos plugins retorna un error se aborta el Scheduling Cycle","title":"Pre Score"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#score","text":"Se utiliza para dar una puntuacion ( scoring) a todos los nodos que han pasado el filtro. Se llama a cada uno de los plugins de score para cada uno de los nodos. Hay un rango definido de enteros para representar el score de un nodo. Despues de la fase de Normalizacion el scheduler combinar\u00e1 la puntuacion de todos los plugins para un nodo segun el peso que cada plugin tenga configurado","title":"Score"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#normalizescore","text":"Sirven para adaptar la puntuacion dada por un Score plugin al intervalo definido. Un plugin que se registra para esta extension solo ser\u00e1 llamado con la informaci\u00f3n que el mismo gener\u00f3 en la fase de Score Esto se llama una vez por plugin y por scheduling cycle. Si un plugin de estos retorna un error se aborta el Scheduling cycle.","title":"NormalizeScore"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#reserve","text":"Los plugins que se registren aqui tienen que tener dos metodos Reserve & Unreserve . Esto se llama antes de que empiece la fase de Binding para evitar condiciones de carrera. Basicamente se les da la oportunidad a los plugins de \u201creservar\u201d ciertos recursos de un nodo para que no sean tenidos en cuenta en los subsiguientes Scheduling cycles aun cuando no se ha producido realmente el binding, para evitar que haya varios Pods luchando por los mismos recursos. El metodo Reserve puede fallar. Si un plugin falla no se llama al resto de los plugins y se considera que la fase ha fallado, abortando el Scheduling cycle. Unreserve se llama si falla la fase de Reserva en un plugin posterior o si falla alguna de las fases posteriores a la de Reserve. Cuando esto pasa se llama al Unreserve de todos los plugins en el orden inverso al de las llamadas a Reverse . La implementacion de los Unreserve tiene que ser idempotente y no puede fallar.","title":"Reserve"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#permit","text":"Es el ultimo punto de ejecucion en el Scheduling Cycle. Permite denegar un scheduling o retrasarlo. Un permit plugin puede hacer una de las tres siguientes cosas: approve : Se env\u00eda al Binding Cycle inmediatamente. deny : Se cancela el Binding y se envia el pod de nuevo a la cola de espera para Scheduling. wait (with timeout) : Env\u00eda el pod a una cola de waiting . El Binding Cycle del pod comienza pero queda inmediatamente bloqueado hasta ser aprovado Si ning\u00fan plugin lo aprueba antes de que pase el timeout el wait se convierte en deny y el Pod se devuelve a la cola de espera para el Scheduling.","title":"Permit"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#binding-cycle","text":"","title":"Binding Cycle"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#prebind","text":"Aqui se debe hacer cualquier trabajo preliminar antes de permitir que el pod se ejecute en el nodo Ej: Montar un volumen de red Si cualquiera de los plugins retorna un error el pod es rejected y enviado a la cola de Scheduling","title":"PreBind"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#bind","text":"Se encargan de hacer el bind de un pod a un nodo ( no tengo ni idea de que implica esto ahora mismo.) Se llama a cada plugin en el orden configurado No son llamados hasta que no han terminado los de PreBinding Un plugin puede elegir si maneja o no un determinado Pod. En el caso de que un plugin elija manejarlo no se sigue llamando a los demas para ese pod","title":"Bind"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#postbind","text":"Esto es basicamente para informar de que el proceso ha terminado Puede utilizarse para hacer limpieza de recursos utilizados en el proceso.","title":"PostBind"},{"location":"pages/kubernetes/scheduler/k8s%20scheduling%20framework/#links","text":"official k8s doc","title":"Links"},{"location":"pages/kubernetes/scheduler/scheduler-back-off/","text":"Esto es que si no hay recursos para poder meterlo en otro nodo se devuelve a la cola para poder intentarlo despues de un rato a ver si hay mas suerte. Estos reintentos cada vez se hacen con una menor frecuencia y entre medias es posible que otro pod de menor prioridad pero que necesita menos recursos pueda ser asignado a un nodo, con lo que se puede dar una especie de \u201cinversion de prioridad\u201d kubernetes scheduling \u00b6","title":"Scheduler back off"},{"location":"pages/kubernetes/scheduler/scheduler-back-off/#kubernetes-scheduling","text":"","title":"kubernetes scheduling"},{"location":"pages/kubernetes/scheduler/priorityclass/Kubernetes%20PriorityClass/","text":"PriorityClass \u00b6 kubernetes scheduling \u00b6 Es un objeto sin namespace es decir clusted scoped Sirve basicamente para hacer un mapping entre un nombre y un valor de prioridad. Cuanto mayor el valor mayor la prioridad El valor de la prioridad puede ser cualquier entero de 32-bits menor o igual a 1 billon los valores por encima de estos estan reservados para pods criticos del sistema Tiene dos campos opcionales: description : Que no es mas que una cadena human readable para los usuarios globalDefault : bool Si es true ser\u00e1 la prioridad que se les asignar\u00e1 a los pods que no tengan Logicamente: solo un objeto PriorityClass puede tener este valor a true al mismo tiempo Si ninguna lo tiene a true se asigna cero como valor por defecto de la prioridad. PriorityClass apiVersion : scheduling.k8s.io/v1 kind : PriorityClass metadata : name : high-priority value : 1000000 globalDefault : false description : \"This priority class should be used for XYZ service pods only.\"","title":"PriorityClass"},{"location":"pages/kubernetes/scheduler/priorityclass/Kubernetes%20PriorityClass/#priorityclass","text":"","title":"PriorityClass"},{"location":"pages/kubernetes/scheduler/priorityclass/Kubernetes%20PriorityClass/#kubernetes-scheduling","text":"Es un objeto sin namespace es decir clusted scoped Sirve basicamente para hacer un mapping entre un nombre y un valor de prioridad. Cuanto mayor el valor mayor la prioridad El valor de la prioridad puede ser cualquier entero de 32-bits menor o igual a 1 billon los valores por encima de estos estan reservados para pods criticos del sistema Tiene dos campos opcionales: description : Que no es mas que una cadena human readable para los usuarios globalDefault : bool Si es true ser\u00e1 la prioridad que se les asignar\u00e1 a los pods que no tengan Logicamente: solo un objeto PriorityClass puede tener este valor a true al mismo tiempo Si ninguna lo tiene a true se asigna cero como valor por defecto de la prioridad. PriorityClass apiVersion : scheduling.k8s.io/v1 kind : PriorityClass metadata : name : high-priority value : 1000000 globalDefault : false description : \"This priority class should be used for XYZ service pods only.\"","title":"kubernetes scheduling"},{"location":"pages/kubernetes/scheduler/priorityclass/non-preempting-priority-class/","text":"Non-preemptive PriorityClass \u00b6 FEATURE STATE: \u2003 Kubernetes v1.24 [stable] info source Hay un campo dentro de la PriorityClass que determina la politica de \u201cpreemtion\u201d de un Pod \u2026 vamos a quien puede el pod echar de un nodo para poder meterse el. El campo es preemptionPolicy El valor por defecto es PreemptLowerPriority Cuando el valor del campo es Never : El pod se pone delante de los pods con menor prioridad en la cola de scheduling ( que entiendo que es para lo que sirve la prioridad) El pod no puede hacer que ninguno otro pod sea echado del nodo para poder entrar el Como todos los demas pods, uno con esta clase est\u00e1 sujeto a scheduler back-off : Evidentemente, los non-preemtive pods aun pueden ser echados de un nodo por otro nodo de mayor prioridad ( y que no tenga esta policita claro) Non-Preemptive PriorityClass apiVersion : scheduling.k8s.io/v1 kind : PriorityClass metadata : name : high-priority-nonpreempting value : 1000000 preemptionPolicy : Never globalDefault : false description : \"This priority class will not cause other pods to be preempted.\"","title":"Non-preemptive PriorityClass"},{"location":"pages/kubernetes/scheduler/priorityclass/non-preempting-priority-class/#non-preemptive-priorityclass","text":"FEATURE STATE: \u2003 Kubernetes v1.24 [stable] info source Hay un campo dentro de la PriorityClass que determina la politica de \u201cpreemtion\u201d de un Pod \u2026 vamos a quien puede el pod echar de un nodo para poder meterse el. El campo es preemptionPolicy El valor por defecto es PreemptLowerPriority Cuando el valor del campo es Never : El pod se pone delante de los pods con menor prioridad en la cola de scheduling ( que entiendo que es para lo que sirve la prioridad) El pod no puede hacer que ninguno otro pod sea echado del nodo para poder entrar el Como todos los demas pods, uno con esta clase est\u00e1 sujeto a scheduler back-off : Evidentemente, los non-preemtive pods aun pueden ser echados de un nodo por otro nodo de mayor prioridad ( y que no tenga esta policita claro) Non-Preemptive PriorityClass apiVersion : scheduling.k8s.io/v1 kind : PriorityClass metadata : name : high-priority-nonpreempting value : 1000000 preemptionPolicy : Never globalDefault : false description : \"This priority class will not cause other pods to be preempted.\"","title":"Non-preemptive PriorityClass"}]}