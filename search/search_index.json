{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"pages/kubernetes/architecture/k8s/","tags":["kubernetes"],"text":"Arquitectura de Kubernetes \u00b6 kubernetes \u00b6 Componentes \u00b6 Control Plane \u00b6 Aqui es donde est\u00e1n los elementos que manejan el comportamiento del cluster pero que no ejecutan nada de las aplicaciones de los containers. Componentes \u00b6 etcd API Server #api-server Scheduler Controller Manager The Nodes ( workers ) \u00b6 Aqui es donde se ejecutan los containers de la aplicaci\u00f3n. Componentes \u00b6 Kubelet Kubernetes Service Proxy (kube-proxy) Conainer Runtime (Docker, rkt \u2026 ) Add-on Components \u00b6 A parte de los componentes del Control Plane y de los nodos se necesitan unas cuantas cosas para que todo funcione bien Kubernetes DNS Server Dashboard Ingress Controller Heapster Container Network Interface network plugin Como se comunican \u00b6 Todos se comunican con el API server. El API server es el \u00fanico que se comunica con la base de datos. Todos modifican el estado de la base de datos hablando con el API server. La comunicaci\u00f3n es siempre comenzada por los otros elementos salvo en algunos casos especiales donde el API server es quien comienza la conexion. Cuando usamos kubectl para obtener los logs Cuando usamos kubectl attach Cuando usamos kubectl port-forward Ejecutando multiples instancias. \u00b6 Todos los elementos que tienen los Worker Nodes tienen que correr en el mismo worker node. ( supongo que en cada nodo tienen que estar todos los elementos) Sin embargo los elementos del control plane pueden ser ejecutados en distintos servidores. Se pueden ejecutar multiples instancias de los elementos del Control plane para tener HA. Sin embargo: Etcd y API server pueden tener varias instancias funcionando en paralelo Scheduler y Controller Manager solo pueden tener una instancia funcionando al mismo tiempo mientras que las dem\u00e1s est\u00e1n en stand-by Como se ejecutan los componentes \u00b6 Todos los componentes del control plane y el kube-proxy pueden ejecutarse en el sistema directamente o pueden ejecutarse como pods en el master node . El unico que tiene que ejecutarse como un componente de sistema siempre es Kubelet Como se usan los componentes \u00b6 Como utiliza K8s ETCD \u00b6 Es el \u00fanico sitio donde se guardan los manifiestos de todos los componentes que se crean en el cluster. Es una base de datos clave-valor distribuida, lo que permite poder tener multiples instancias para tener HA El unico que se comunica con el etcd es el API Server. Esto tiene unas ventajas Se puede cambiar la base de datos con facilidad adaptando el API server. Puede implementarse un sistema de locking optimista Se puede hacer validacion de los elementos antes de guardarlos o de recuperarlos. Como se guardan las cosas en etcd \u00b6 Hay dos versiones en uso v2 y v3 ( aunque seguramente ahora la v2 ya no se use porque la v3 tiene mejor performance.) En v2 Las claves son jerarquicas y estan guardadas en arbol, por lo que puede pensarse en la clave de una entrada como en el nombre completo de un fichero en un arbol de directorios. Por tanto cada clave es: O bien un directorio que a su vez contiene otras claves O bien una clave regular que contiene un valor. En v3 No soporta el concepto de directorios per se, pero como las claves siguen siendo organizadas de manera jer\u00e1rquica y se pueden poner / , podemos seguir pensando en ellas como el nombre completo de un fichero dentro de un filesystem. Todos los datos que kubernetes guarda en etcd se guardan debajo de /registry title: Listado de entradas en etcd V2 ```bash $ etcdctl ls /registry ``` V3: Las entradas cuya clave empieza por un prefijo determinado. ```bash $ etcdtl get /registry --prefix=true ``` Salida ```bash /registry/configmaps /registry/daemonsets /registry/deployments /registry/events /registry/namespaces /registry/pods ... ``` title : pods en el namespace default collapse: true ```bash $ etcdctl ls /registry/pods/default /registry/pods/default/kubia-159041347-xk0vc /registry/pods/default/kubia-159041347-wt6ga /registry/pods/default/kubia-159041347-hp2o5 ``` y esto es lo que se guarda de una entrada en particular ```bash $ etcdctl get /registry/pods/default/kubia-159041347-wt6ga {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\", \"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":... ``` Como puede verse es un documento en formato json. Consistencia en ETCD cuando hay un cluster. \u00b6 Como podemos tener multiples instancias de etcd en un cluster tenemos que asegurar que hay consistencia en el estado que muestran todas. Para esto se utiliza el [[Algoritmo RAFT]] Es un algoritmo de consenso, de manera que para transicionar a un nuevo estado, mas de la mitad de los nodos tienen que estar de acuerdo. En este algoritmo se asegura que una determinada instancia de etcd o bien muestra el ultimo estado al que se ha llegado o bien muestra uno de los antiguos estados a los que se llego por consenso. Si se produce un split brain, si una de las particiones tiene la mayoria de los nodos evidentemente la otra no los tendr\u00e1. Por tanto la primera particion si que podr\u00e1 transicionar de estado porque tienen la mayoria, mientras que la otra no podr\u00e1 modificar el estado del cluster, qued\u00e1ndose en un estado v\u00e1lido., Cuando la situacion se solucione los nodos que estaban en un estado antiguo, pero valido, podr\u00e1n recuperarse y hacer catch-up. Debido a la necesidad de quorum es mejor siempre tener un numero impar de instancias de etcd. Teniendo un numero par no aumentamos el numero de fallos de instancias a los que somos resilientes, pero si que aumentamos la posibilidad de fallo, ya que a mas instancias en el cluster mas posibilidades de que alguna falle. Normalmente los clusters mas grandes de etcd tienen CINCO o SIETE instancias, lo que permite manejar sin problemas el fallo de DOS o TRES instancias, lo que suele ser suficiente en la mayoria de los casos. Que hace el API server \u00b6 Intro \u00b6 Es el componente que todos los demas componentes, incluido el kubectl utilizan para poder interactuar con el sistema. Provee un interface CRUD sobre un API Rest Guarda todos los datos en ETCD Provee consistencia en los datos que se guardan Chequea los datos que los demas componentes quieren guardar en etcd antes de guardarlos para evitar estados inconsistentes del sistema Implementa un Optimistic Locking system Dado que podemos tener multiples instancias de API server ejecutandose de manera concurrente tenemos que evitar que los cambios puedan sobreescribir otros cambios que se est\u00e9n haciendo en ese momento 'kubectl' es uno de los clientes del API server. Cuando creamos un nuevo recurso con un fichero `yaml` lo que hace es enviar un HTTP POST con el fichero en formato json y esto es lo que pasa en el API Server ![[APIServerHTTPPOST.png]] Stages \u00b6 Authentication \u00b6 Lo primero que se hace es autenticar al cliente que envia la peticion. Se envia la peticion a una lista de plugins de autenticacion que esten configurados en el API server, pasando por todos hasta que uno de ellos sea capaz de determinar quien es el cliente que envia la peticion. Authorization \u00b6 Una vez que sabemos quien es el cliente que pide la accion y que podemos confiar en el, tenemos que ver si dicho cliente tiene autorizacion para poder hacer dicha accion sobre el recurso que esta pidiendo hacerla. Para esto estan los plugins de autorizacion. La peticion va pasando por los plugins hasta que uno de ellos dice que el cliente puede realizar al accion sobre el recurso pedido. Admision Control \u00b6 Las operaciones de Creacion, Modificacion y Borrado de recursos pasan por una fase adicional, la de Control de Admision. En esta fase la peticion pasas por todos los plugins . Estos plugins pueden modificar la peticion A\u00f1adiendo valores por defecto que no existen en el recurso de la peticion Cambiando valores del recursos que exceden limites Incluso pueden modificar recursos relacionados que no est\u00e9n en la peticion Tambien pueden validad y rechazar la peticion por el contenido que tiene. title: Ejemplos de Admission Control Plugins. - `AlwaysPullImages` -> Sobreescribe el `imagePullPolicy` de los Pods y lo pone a `Always` - `ServiceAccount` -> Aplica un `ServiceAccount` por defecto a todos los Pods que no definen uno. - `NamespaceLifecycle` -> Evita que se puedan crear Pods en namespaces que est\u00e1n en proceso de borrado o que no existen. - `ResourceQuota` -> Se asegura que los Pods de un determinado namespace solo puedan utilizar unas cantidades de CPU y memoria determinadas How API Server Notifies clients. \u00b6 El API server permite que los demas componentes pidan ser notificados cuando un recurso es Creado Modificado o Borrado, de manera que pueda hacer lo que sea que tenga que hacer en respuesta a esta operacion. El Cliente abre una conexion HTTP al API server y a trav\u00e9s de dicha conexion el cliente recive un stream de moficiaciones. Cada vez que uno de los objetos observados cambia se le envia al cliente la nueva version del objeto. Este mecanismo lo usa por ejemplo el kubectl cuando pedimos que nos haga un watch de los pods kubectl get pods -o yaml --watch Tambien lo utiliza el Scheduler Entendiendo el funcionamiento del Scheduler \u00b6 Controllers ejecutandose en el Controller Manager. \u00b6 Intro \u00b6 Hasta ahora tenemos en el control plane: ETCD: que solo guarda los manifiestos. API Server: que es un interface para que los demas puedan guardar cosas en la base de datos y ser notificados Scheduler: Que se encarga de asingar nodos a los nuevos pods. Pero no tenemos nadie que haga nada para mantener el estado del cluster conciliado con el estado de los manifiestos guardados en la base de datos. Esto es lo que hace el Controller Manager, o para ser mas exactlos la lista de controllers que se ejecutan en el. Todos los controller se al formar parte del Control Plane, se ejecutan en el Master Node Controllers en general \u00b6 Actualmente ( no tengo claro que eso sea cierto ahora mismo) un solo proceso Controller Manager combina dentro de el varios controllers distintos que realizan distintas labores. Eventualmente cada uno de estos controllers ser\u00e1 separado en un proceso distinto para que sea mas sencillo cambiarlos. Lista de Controllers: Replication Manager \u2013 Un controller para ReplicationController resources ReplicaSet Controller DaemonSet Controller Job Controller Deployment Controller StatefulSet Controller Node Controller Service Controller Endpoints Controller Namespace Controller PersistentVolumeController Others \u2026 que cachondo. Basicamente hay un Controller por cada uno de los recursos de kubernetes. Los recursos son una declaracion de cual queremos que sea el estado de un cluster. Los Controllers realizan el trabajo de intentar conciliar el estado actual del cluster con el que queremos que sea. En general los controllers tienen un \u201creconciliation loop\u201d donde intentan hacer esta reconciliacion del estado creando, modificando o borrando determinados recursos. collapse:true En cada recurso el `spec` es donde ponemos el estado esperado mientras que los controllers escriben en el `status` el estado actual. Utilizan el sistema de notificacion del API server para saber cuando se modifican los recursos a los que atienden. Tambien tienen una operacion \u201cre-list\u201d que ejecutan periodicamente por si acaso han perdido alguna notificacion. Los Controllers no hablan nunca unos con otros. Los Controllers no saben ni que existen otros controllers. Solo hablan con el API server nada mas. Cada uno hace su parte y esperan que todo funcione correctamente. Algunos controllers \u00b6 Replication Manager \u00b6 Basicamente espera ser notificado de los Pods y los ReplicationController que se van creando, destruyendo o modificando para comprobar si lo que se ha puesto en los ReplicationController se sigue cumpliendo. En el caso de que el n\u00famero de pods no sea el esperado realiza las operaciones adecuadas para poder llegar al estado deseado. Si necesita crear nuevos pods utiliza el spec.template del ReplicationController para enviar un HTTP POST al API Server con el nuevo manifiesto para que se desencadene la creacion de un nuevo pod. El Replication Manager hace su trabajo solo modificando los manifiestos de los recursos mediante el API server como hacen todos los demas controllers ReplicaSet Controller, DaemonSet Controller y Job Controller \u00b6 Los tres hacen basicamente lo mismo que el Replication Manager, solo que usando distintos recursos\u2026 Kubernetes ReplicaSet , Kubernetes DaemonSet y Kubernetes Job Deployment Controller \u00b6 Gestiona las actualizaciones de los Deployment . Hace un roll-out del deployment cuando se modifica. Basicamente crea un nuevo Replicaset y despues realiza una escalacion tanto del antiguo como del nuevo Replicaset basandose en la estrategia definida en el Deployment hasta que todos los viejos Pods han desaparecido y han sido reemplazados por los nuevos. StatefulSet Controller \u00b6 Como los demas Controllers el StatefulSet Controller crea y destruye pods de acuerdo con la especificacion de los StatefulSet . Adem\u00e1s es el \u00fanico que tambien maneja los PersistentVolumeClaims para cada instancia de Pod Node Controller \u00b6 Maneja los Node Resources que describen los workers de un cluster. Entre otras cosas mantiene una lista actualizada de los nodos del cluster monitorizando las maquinas que se a\u00f1aden o quitan. Tambien monitoriza la salud de los nodos y elimina los Pods de los nodos que no pueden ser alcanzados. Service Controller \u00b6 Cuando un Kubernetes Service de tipo LoadBalancer se crea se pide un LoadBalancer al sistema para poder hacer el Servicio visible desde el exterior. Este controler es el encargado de realizar la operacion de pedir el recurso y de liberarlo cuando no es necesario. Endpoints Controller \u00b6 Los Kubernetes Service no son enlazados directamente a los Pods, si no que contienen una lista de los endpotins (IP + Puerto), lista que es actualizada de acuerdo con el pod selector del Kubernetes Service El Endpoints Controller es el encargado de mantener la lista de endpoints constantemente actualizada con las IPs y puertos de los Pods que matchean el label selector Namespace Controller \u00b6 Cuando un namespace es borrado todos los recursos que pertenezcan a dicho namespace deben ser borrados. Esto es lo que hace el controller. PersistentVolume Controller \u00b6 Cuando un usuario crea un Kubernetes PersistentVolumeClaims kubernetes tiene que encontrar el Kubernetes PersistentVolume al que poder enlazarlo. Esto es justamente lo que hace este controller. Cuando se crea un nuevo Kubernetes PersistentVolumeClaims este controller intenta encontrar el mejor match para el. Busca el Kubernetes PersistentVolume mas peque\u00f1o que tenga el modo de acceso adecuado y que tenga una capacidad suficiente para el claim. Para esto mantiene una lista ordenada de Kubernetes PersistentVolume para cada uno de los modos de acceso por orden ascendente de capacidad ( asi que solo tiene que devolver el primero de la lista.) Cuando el Kubernetes PersistentVolumeClaims es borrado el Kubernetes PersistentVolume es reclamado por el controller de acuerdo con la politica de reclaim \u2026 ( puede quedarse como est\u00e1, ser borrado, ser vaciado\u2026) Kubelet \u00b6 Intro \u00b6 Al contrario que los controllers tango kubelet como kube-proxy se ejecutan en los worker nodes, que es donde se ejecutan los containers de las aplicaciones. Responsabilidades \u00b6 Kubelet es el responsable de todo lo que sucede en un worker node. Lo primero que tiene que hacer es registrar el nodo en el cluster, para eso crea un Kubernetes Node Resources usando el API Server. A partir de ese momento monitoriza todos los Pods para ver cuales son asignados por el Scheduler para ser ejecutados en su nodo, y cuando esto sucede habla con el Container Runtime que haya configurado para que se ejecuten los containers del Pod. Tambi\u00e9n monitoriza constantemente los containers de los Pods que est\u00e1n ejecutandose en su Nodo para poder reportar su estatus, eventos y consumo de recursos al API Server. Kubelet tambien es el responsable de ejecutar los liveness probes, rearrancar los containers cuando la probe falle y por \u00faltimo ser quien borre los contenedores cuando el Pod sea borrado. Ejecutando Pods staticos sin el API Server. \u00b6 Normalmente kubelet obtiene los manifiestos de los pods que tiene que ejecutar del API server, pero tambien puede ejecutar otros Pods obteniendo los manifiestos de un directorio del Nodo en el que se ejecuta. Esto se utiliza principalmente para poder ejecutar componentes del Control Plane como Pods en lugar de como procesos del master node. Kubernetes Service Proxy \u00b6 Los Worker Nodes tambien ejecutan el kube-proxy. El \u00fanico cometido de kube-proxy es asegurarse que cuando llamamos a uno de los Kubernetes Service la llamada termina en uno de los Pods que est\u00e1 dando soporte a dicho servicio. Cuando hay mas de un Pod dando soporte a dicho servicio se realiza un load-balancing. Se llama kube-proxy porque en la primera de las implementaciones era un proceso en espacio de usuario que hab\u00eda modificado las IP tables para que las peticiones a los servicios fueran redirigidas a \u00e9l y luego fuese el quien las reenviase a los Pods adecuados. Todas las conexiones pasaban por \u00e9l. Ahora hay una implementaci\u00f3n mucho mejor en performance que simplemente modifica la configuracion de las IP tables para que las conexiones vayan a parar al Pod directamente sin pasar por el proceso kube-proxy , por lo que ya no es un proxy estrictamente hablando, pero el nombre se le ha quedado. La diferencia principal entre los dos modos es que en el antiguo al tener que pasar todos los mensajes por el kube-proxy todos tenian que pasar por el espacio de usuario, lo que era una putada para el performance. Ademas el antiguo hac\u00eda un round robin entre los pods de un servicio mientras que el nuevo lo hace de manera aleatoria. Kubernetes Add-ons \u00b6 Hay algunos otros componentes que, sin ser extrictamente necesarios, permiten realizar acciones interesantes en el cluster. Son los add-ons. Estos componentes se instalan en el sistema como Pods, enviando sus manifiestos al API server como cualquier otro pod. Algunos de ellos se instalan mediante Deployment u otro tipo de recurso de k8s title: Algunos add-ons Podemos ver como en Minikube el Ingress Controller y el Dashboard son instalados mediante un [[Kubernetes ReplicationController|ReplicationController]] ```bash $ kubectl get rc -n kube-system NAME DESIRED CURRENT READY AGE default-http-backend 1 1 1 6d kubernetes-dashboard 1 1 1 6d nginx-ingress-controller 1 1 1 6d ``` Mientras que el DNS es un [Deployment](../../../Kubernetes%20Deployment \"Deployment\") ```bash $ kubectl get deploy -n kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-dns 1 1 1 1 6d Como funciona el DNS server \u00b6 Todos los pods en un cluster estan configurados para utilizar el DNS interno por defecto. El DNS se expone mediante un servicio kube-dns para poder asi mover los pods dentro del cluster sin problema. La direccion IP del servicio esta configurada como el nameserver en el fichero /etc/resolv.conf dentro de cada container instalado en el cluster. kube-dns utiliza el mecanismo de notificaciones del API server para observar cambios en los Kubernetes Service y en los Endpoints y actualiza sus tablas internas con cada uno de los cambios. Durante el momento en el que se actualiza un Kubernetes Service o un Kubernetes Endpoint hasta que el kube-dns recibe la notificacion y actualiza sus registros puede que haya informaci\u00f3n erronea en el DNS Ejemplo de como funciona todo junto \u00b6 Todo kubernetes esta formado por peque\u00f1os elementos muy poco acoplados con una buena separacion de responsabilidades ( cada uno hace una cosa y solo una puta cosa) y esa es la raz\u00f3n por la que todo funciona bien Vamos a ver como funcionan todas las piezas en armon\u00eda para la creacion de un Pod. Normalmente no se crea un Pod desde cero, de modo que vamos a ver como se realiza la creacion de un Pod, pero partiendo del manifiesto de un Deployment. Antes siquiera de empezar ya tenemos la siguiente figura, donde cada uno de los componentes est\u00e1 escuchando las notificaciones adecuadas del API server respecto a los recursos que le son interesantes ( normalmente solo uno o un par a lo sumo) Bien, una vez que kubectl en via mediante un HTTP POST el manifiesto del deployment al API Server esta es la cadena de acontecimientos que se desata. Se pueden ver todos los eventos que van sucediendo en el cluster mediante el siguiente comando de kubectl kubectl get events --watch Entendiendo que pasa cuando se ejecuta un POD \u00b6 Cuando ejecutamos un Pod en uno de los nodos no solo se ejecutan los containers que tenemos definidos en el pod, si no que ademas se ejecuta otro pod. Supongamos que solo tenemos un container en el pod para hacerlo mas sencillo. $ kubectl run nginx --image = nginx deployment \"nginx\" created Si vamos al nodo en el que se est\u00e1 ejecutando el pod podemos ver los siguientes contenedores ejecutandose. docker@minikubeVM:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED c917a6f3c3f7 nginx \"nginx -g 'daemon off\" 4 seconds ago 98b8bf797174 gcr.io/.../pause:3.0 \"/pause\" 7 seconds ago Podemos ver que hay un contenedor adicional ejecutandose Dicho contenedor no est\u00e1 ejecutando nada porque esta en pause Ademas podemos ver que se ha creado antes que el contenedor que teniamos definido en el pod Ese contenedor es parte de la infraestructura Todos los contenedores de un mismo pod comparten algunos linux namespaces como por ejemplo el espacio de red \u2026 Este contenedor es el que contiene todos los linux namespaces que son compartidos por todos los contenedores de un mismo pod. Todos los contenedores del mismo pod tienen que compartir los mismos linux namespaces aunque se reinicien, pero eso es posible porque estos est\u00e1n en este container, de manera que aunque los dem\u00e1s se reinicien los namespaces se mantienen. El ciclo de vida de este contenedor esta ligado intimamente al del pod. Si por cualquier motivo este container resultase parado, kubelet volver\u00eda a recrearlo y despu\u00e9s recrear\u00eda todos los demas containers del pod para poder asignarles los linux namespaces del nuevo container. Interpod networking \u00b6 Cada pod tiene un IP unica Todos los pods pueden comunicarse entre si mediante conexiones \u201cplanas\u201d es decir sin NAT Kubernetes no es quien configura esto, es el administrador quien tiene que conseguirlo. Kubernetes solo pone el requisito. La red tiene que estar configurada de tal manera que la IP que un Pod ve de si mismo sea la misma que los dem\u00e1s pueden ver de \u00e9l. Todos los containers deben de poder comunicarse entre ellos independientemente de si estan siendo ejecutados en el mismo o en distintos nodos.","title":"Arquitectura de Kubernetes"},{"location":"pages/kubernetes/architecture/k8s/#arquitectura-de-kubernetes","text":"","title":"Arquitectura de Kubernetes"},{"location":"pages/kubernetes/architecture/k8s/#kubernetes","text":"","title":"kubernetes"},{"location":"pages/kubernetes/architecture/k8s/#componentes","text":"","title":"Componentes"},{"location":"pages/kubernetes/architecture/k8s/#control-plane","text":"Aqui es donde est\u00e1n los elementos que manejan el comportamiento del cluster pero que no ejecutan nada de las aplicaciones de los containers.","title":"Control Plane"},{"location":"pages/kubernetes/architecture/k8s/#componentes_1","text":"etcd API Server #api-server Scheduler Controller Manager","title":"Componentes"},{"location":"pages/kubernetes/architecture/k8s/#the-nodes-workers","text":"Aqui es donde se ejecutan los containers de la aplicaci\u00f3n.","title":"The Nodes ( workers )"},{"location":"pages/kubernetes/architecture/k8s/#componentes_2","text":"Kubelet Kubernetes Service Proxy (kube-proxy) Conainer Runtime (Docker, rkt \u2026 )","title":"Componentes"},{"location":"pages/kubernetes/architecture/k8s/#add-on-components","text":"A parte de los componentes del Control Plane y de los nodos se necesitan unas cuantas cosas para que todo funcione bien Kubernetes DNS Server Dashboard Ingress Controller Heapster Container Network Interface network plugin","title":"Add-on Components"},{"location":"pages/kubernetes/architecture/k8s/#como-se-comunican","text":"Todos se comunican con el API server. El API server es el \u00fanico que se comunica con la base de datos. Todos modifican el estado de la base de datos hablando con el API server. La comunicaci\u00f3n es siempre comenzada por los otros elementos salvo en algunos casos especiales donde el API server es quien comienza la conexion. Cuando usamos kubectl para obtener los logs Cuando usamos kubectl attach Cuando usamos kubectl port-forward","title":"Como se comunican"},{"location":"pages/kubernetes/architecture/k8s/#ejecutando-multiples-instancias","text":"Todos los elementos que tienen los Worker Nodes tienen que correr en el mismo worker node. ( supongo que en cada nodo tienen que estar todos los elementos) Sin embargo los elementos del control plane pueden ser ejecutados en distintos servidores. Se pueden ejecutar multiples instancias de los elementos del Control plane para tener HA. Sin embargo: Etcd y API server pueden tener varias instancias funcionando en paralelo Scheduler y Controller Manager solo pueden tener una instancia funcionando al mismo tiempo mientras que las dem\u00e1s est\u00e1n en stand-by","title":"Ejecutando multiples instancias."},{"location":"pages/kubernetes/architecture/k8s/#como-se-ejecutan-los-componentes","text":"Todos los componentes del control plane y el kube-proxy pueden ejecutarse en el sistema directamente o pueden ejecutarse como pods en el master node . El unico que tiene que ejecutarse como un componente de sistema siempre es Kubelet","title":"Como se ejecutan los componentes"},{"location":"pages/kubernetes/architecture/k8s/#como-se-usan-los-componentes","text":"","title":"Como se usan los componentes"},{"location":"pages/kubernetes/architecture/k8s/#como-utiliza-k8s-etcd","text":"Es el \u00fanico sitio donde se guardan los manifiestos de todos los componentes que se crean en el cluster. Es una base de datos clave-valor distribuida, lo que permite poder tener multiples instancias para tener HA El unico que se comunica con el etcd es el API Server. Esto tiene unas ventajas Se puede cambiar la base de datos con facilidad adaptando el API server. Puede implementarse un sistema de locking optimista Se puede hacer validacion de los elementos antes de guardarlos o de recuperarlos.","title":"Como utiliza K8s ETCD"},{"location":"pages/kubernetes/architecture/k8s/#como-se-guardan-las-cosas-en-etcd","text":"Hay dos versiones en uso v2 y v3 ( aunque seguramente ahora la v2 ya no se use porque la v3 tiene mejor performance.) En v2 Las claves son jerarquicas y estan guardadas en arbol, por lo que puede pensarse en la clave de una entrada como en el nombre completo de un fichero en un arbol de directorios. Por tanto cada clave es: O bien un directorio que a su vez contiene otras claves O bien una clave regular que contiene un valor. En v3 No soporta el concepto de directorios per se, pero como las claves siguen siendo organizadas de manera jer\u00e1rquica y se pueden poner / , podemos seguir pensando en ellas como el nombre completo de un fichero dentro de un filesystem. Todos los datos que kubernetes guarda en etcd se guardan debajo de /registry title: Listado de entradas en etcd V2 ```bash $ etcdctl ls /registry ``` V3: Las entradas cuya clave empieza por un prefijo determinado. ```bash $ etcdtl get /registry --prefix=true ``` Salida ```bash /registry/configmaps /registry/daemonsets /registry/deployments /registry/events /registry/namespaces /registry/pods ... ``` title : pods en el namespace default collapse: true ```bash $ etcdctl ls /registry/pods/default /registry/pods/default/kubia-159041347-xk0vc /registry/pods/default/kubia-159041347-wt6ga /registry/pods/default/kubia-159041347-hp2o5 ``` y esto es lo que se guarda de una entrada en particular ```bash $ etcdctl get /registry/pods/default/kubia-159041347-wt6ga {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\", \"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":... ``` Como puede verse es un documento en formato json.","title":"Como se guardan las cosas en etcd"},{"location":"pages/kubernetes/architecture/k8s/#consistencia-en-etcd-cuando-hay-un-cluster","text":"Como podemos tener multiples instancias de etcd en un cluster tenemos que asegurar que hay consistencia en el estado que muestran todas. Para esto se utiliza el [[Algoritmo RAFT]] Es un algoritmo de consenso, de manera que para transicionar a un nuevo estado, mas de la mitad de los nodos tienen que estar de acuerdo. En este algoritmo se asegura que una determinada instancia de etcd o bien muestra el ultimo estado al que se ha llegado o bien muestra uno de los antiguos estados a los que se llego por consenso. Si se produce un split brain, si una de las particiones tiene la mayoria de los nodos evidentemente la otra no los tendr\u00e1. Por tanto la primera particion si que podr\u00e1 transicionar de estado porque tienen la mayoria, mientras que la otra no podr\u00e1 modificar el estado del cluster, qued\u00e1ndose en un estado v\u00e1lido., Cuando la situacion se solucione los nodos que estaban en un estado antiguo, pero valido, podr\u00e1n recuperarse y hacer catch-up. Debido a la necesidad de quorum es mejor siempre tener un numero impar de instancias de etcd. Teniendo un numero par no aumentamos el numero de fallos de instancias a los que somos resilientes, pero si que aumentamos la posibilidad de fallo, ya que a mas instancias en el cluster mas posibilidades de que alguna falle. Normalmente los clusters mas grandes de etcd tienen CINCO o SIETE instancias, lo que permite manejar sin problemas el fallo de DOS o TRES instancias, lo que suele ser suficiente en la mayoria de los casos.","title":"Consistencia en ETCD cuando hay un cluster."},{"location":"pages/kubernetes/architecture/k8s/#que-hace-el-api-server","text":"","title":"Que hace el API server"},{"location":"pages/kubernetes/architecture/k8s/#intro","text":"Es el componente que todos los demas componentes, incluido el kubectl utilizan para poder interactuar con el sistema. Provee un interface CRUD sobre un API Rest Guarda todos los datos en ETCD Provee consistencia en los datos que se guardan Chequea los datos que los demas componentes quieren guardar en etcd antes de guardarlos para evitar estados inconsistentes del sistema Implementa un Optimistic Locking system Dado que podemos tener multiples instancias de API server ejecutandose de manera concurrente tenemos que evitar que los cambios puedan sobreescribir otros cambios que se est\u00e9n haciendo en ese momento 'kubectl' es uno de los clientes del API server. Cuando creamos un nuevo recurso con un fichero `yaml` lo que hace es enviar un HTTP POST con el fichero en formato json y esto es lo que pasa en el API Server ![[APIServerHTTPPOST.png]]","title":"Intro"},{"location":"pages/kubernetes/architecture/k8s/#stages","text":"","title":"Stages"},{"location":"pages/kubernetes/architecture/k8s/#authentication","text":"Lo primero que se hace es autenticar al cliente que envia la peticion. Se envia la peticion a una lista de plugins de autenticacion que esten configurados en el API server, pasando por todos hasta que uno de ellos sea capaz de determinar quien es el cliente que envia la peticion.","title":"Authentication"},{"location":"pages/kubernetes/architecture/k8s/#authorization","text":"Una vez que sabemos quien es el cliente que pide la accion y que podemos confiar en el, tenemos que ver si dicho cliente tiene autorizacion para poder hacer dicha accion sobre el recurso que esta pidiendo hacerla. Para esto estan los plugins de autorizacion. La peticion va pasando por los plugins hasta que uno de ellos dice que el cliente puede realizar al accion sobre el recurso pedido.","title":"Authorization"},{"location":"pages/kubernetes/architecture/k8s/#admision-control","text":"Las operaciones de Creacion, Modificacion y Borrado de recursos pasan por una fase adicional, la de Control de Admision. En esta fase la peticion pasas por todos los plugins . Estos plugins pueden modificar la peticion A\u00f1adiendo valores por defecto que no existen en el recurso de la peticion Cambiando valores del recursos que exceden limites Incluso pueden modificar recursos relacionados que no est\u00e9n en la peticion Tambien pueden validad y rechazar la peticion por el contenido que tiene. title: Ejemplos de Admission Control Plugins. - `AlwaysPullImages` -> Sobreescribe el `imagePullPolicy` de los Pods y lo pone a `Always` - `ServiceAccount` -> Aplica un `ServiceAccount` por defecto a todos los Pods que no definen uno. - `NamespaceLifecycle` -> Evita que se puedan crear Pods en namespaces que est\u00e1n en proceso de borrado o que no existen. - `ResourceQuota` -> Se asegura que los Pods de un determinado namespace solo puedan utilizar unas cantidades de CPU y memoria determinadas","title":"Admision Control"},{"location":"pages/kubernetes/architecture/k8s/#how-api-server-notifies-clients","text":"El API server permite que los demas componentes pidan ser notificados cuando un recurso es Creado Modificado o Borrado, de manera que pueda hacer lo que sea que tenga que hacer en respuesta a esta operacion. El Cliente abre una conexion HTTP al API server y a trav\u00e9s de dicha conexion el cliente recive un stream de moficiaciones. Cada vez que uno de los objetos observados cambia se le envia al cliente la nueva version del objeto. Este mecanismo lo usa por ejemplo el kubectl cuando pedimos que nos haga un watch de los pods kubectl get pods -o yaml --watch Tambien lo utiliza el Scheduler","title":"How API Server Notifies clients."},{"location":"pages/kubernetes/architecture/k8s/#entendiendo-el-funcionamiento-del-scheduler","text":"","title":"Entendiendo el funcionamiento del Scheduler"},{"location":"pages/kubernetes/architecture/k8s/#controllers-ejecutandose-en-el-controller-manager","text":"","title":"Controllers ejecutandose en el Controller Manager."},{"location":"pages/kubernetes/architecture/k8s/#intro_1","text":"Hasta ahora tenemos en el control plane: ETCD: que solo guarda los manifiestos. API Server: que es un interface para que los demas puedan guardar cosas en la base de datos y ser notificados Scheduler: Que se encarga de asingar nodos a los nuevos pods. Pero no tenemos nadie que haga nada para mantener el estado del cluster conciliado con el estado de los manifiestos guardados en la base de datos. Esto es lo que hace el Controller Manager, o para ser mas exactlos la lista de controllers que se ejecutan en el. Todos los controller se al formar parte del Control Plane, se ejecutan en el Master Node","title":"Intro"},{"location":"pages/kubernetes/architecture/k8s/#controllers-en-general","text":"Actualmente ( no tengo claro que eso sea cierto ahora mismo) un solo proceso Controller Manager combina dentro de el varios controllers distintos que realizan distintas labores. Eventualmente cada uno de estos controllers ser\u00e1 separado en un proceso distinto para que sea mas sencillo cambiarlos. Lista de Controllers: Replication Manager \u2013 Un controller para ReplicationController resources ReplicaSet Controller DaemonSet Controller Job Controller Deployment Controller StatefulSet Controller Node Controller Service Controller Endpoints Controller Namespace Controller PersistentVolumeController Others \u2026 que cachondo. Basicamente hay un Controller por cada uno de los recursos de kubernetes. Los recursos son una declaracion de cual queremos que sea el estado de un cluster. Los Controllers realizan el trabajo de intentar conciliar el estado actual del cluster con el que queremos que sea. En general los controllers tienen un \u201creconciliation loop\u201d donde intentan hacer esta reconciliacion del estado creando, modificando o borrando determinados recursos. collapse:true En cada recurso el `spec` es donde ponemos el estado esperado mientras que los controllers escriben en el `status` el estado actual. Utilizan el sistema de notificacion del API server para saber cuando se modifican los recursos a los que atienden. Tambien tienen una operacion \u201cre-list\u201d que ejecutan periodicamente por si acaso han perdido alguna notificacion. Los Controllers no hablan nunca unos con otros. Los Controllers no saben ni que existen otros controllers. Solo hablan con el API server nada mas. Cada uno hace su parte y esperan que todo funcione correctamente.","title":"Controllers en general"},{"location":"pages/kubernetes/architecture/k8s/#algunos-controllers","text":"","title":"Algunos controllers"},{"location":"pages/kubernetes/architecture/k8s/#replication-manager","text":"Basicamente espera ser notificado de los Pods y los ReplicationController que se van creando, destruyendo o modificando para comprobar si lo que se ha puesto en los ReplicationController se sigue cumpliendo. En el caso de que el n\u00famero de pods no sea el esperado realiza las operaciones adecuadas para poder llegar al estado deseado. Si necesita crear nuevos pods utiliza el spec.template del ReplicationController para enviar un HTTP POST al API Server con el nuevo manifiesto para que se desencadene la creacion de un nuevo pod. El Replication Manager hace su trabajo solo modificando los manifiestos de los recursos mediante el API server como hacen todos los demas controllers","title":"Replication Manager"},{"location":"pages/kubernetes/architecture/k8s/#replicaset-controller-daemonset-controller-y-job-controller","text":"Los tres hacen basicamente lo mismo que el Replication Manager, solo que usando distintos recursos\u2026 Kubernetes ReplicaSet , Kubernetes DaemonSet y Kubernetes Job","title":"ReplicaSet Controller, DaemonSet Controller y Job Controller"},{"location":"pages/kubernetes/architecture/k8s/#deployment-controller","text":"Gestiona las actualizaciones de los Deployment . Hace un roll-out del deployment cuando se modifica. Basicamente crea un nuevo Replicaset y despues realiza una escalacion tanto del antiguo como del nuevo Replicaset basandose en la estrategia definida en el Deployment hasta que todos los viejos Pods han desaparecido y han sido reemplazados por los nuevos.","title":"Deployment Controller"},{"location":"pages/kubernetes/architecture/k8s/#statefulset-controller","text":"Como los demas Controllers el StatefulSet Controller crea y destruye pods de acuerdo con la especificacion de los StatefulSet . Adem\u00e1s es el \u00fanico que tambien maneja los PersistentVolumeClaims para cada instancia de Pod","title":"StatefulSet Controller"},{"location":"pages/kubernetes/architecture/k8s/#node-controller","text":"Maneja los Node Resources que describen los workers de un cluster. Entre otras cosas mantiene una lista actualizada de los nodos del cluster monitorizando las maquinas que se a\u00f1aden o quitan. Tambien monitoriza la salud de los nodos y elimina los Pods de los nodos que no pueden ser alcanzados.","title":"Node Controller"},{"location":"pages/kubernetes/architecture/k8s/#service-controller","text":"Cuando un Kubernetes Service de tipo LoadBalancer se crea se pide un LoadBalancer al sistema para poder hacer el Servicio visible desde el exterior. Este controler es el encargado de realizar la operacion de pedir el recurso y de liberarlo cuando no es necesario.","title":"Service Controller"},{"location":"pages/kubernetes/architecture/k8s/#endpoints-controller","text":"Los Kubernetes Service no son enlazados directamente a los Pods, si no que contienen una lista de los endpotins (IP + Puerto), lista que es actualizada de acuerdo con el pod selector del Kubernetes Service El Endpoints Controller es el encargado de mantener la lista de endpoints constantemente actualizada con las IPs y puertos de los Pods que matchean el label selector","title":"Endpoints Controller"},{"location":"pages/kubernetes/architecture/k8s/#namespace-controller","text":"Cuando un namespace es borrado todos los recursos que pertenezcan a dicho namespace deben ser borrados. Esto es lo que hace el controller.","title":"Namespace Controller"},{"location":"pages/kubernetes/architecture/k8s/#persistentvolume-controller","text":"Cuando un usuario crea un Kubernetes PersistentVolumeClaims kubernetes tiene que encontrar el Kubernetes PersistentVolume al que poder enlazarlo. Esto es justamente lo que hace este controller. Cuando se crea un nuevo Kubernetes PersistentVolumeClaims este controller intenta encontrar el mejor match para el. Busca el Kubernetes PersistentVolume mas peque\u00f1o que tenga el modo de acceso adecuado y que tenga una capacidad suficiente para el claim. Para esto mantiene una lista ordenada de Kubernetes PersistentVolume para cada uno de los modos de acceso por orden ascendente de capacidad ( asi que solo tiene que devolver el primero de la lista.) Cuando el Kubernetes PersistentVolumeClaims es borrado el Kubernetes PersistentVolume es reclamado por el controller de acuerdo con la politica de reclaim \u2026 ( puede quedarse como est\u00e1, ser borrado, ser vaciado\u2026)","title":"PersistentVolume Controller"},{"location":"pages/kubernetes/architecture/k8s/#kubelet","text":"","title":"Kubelet"},{"location":"pages/kubernetes/architecture/k8s/#intro_2","text":"Al contrario que los controllers tango kubelet como kube-proxy se ejecutan en los worker nodes, que es donde se ejecutan los containers de las aplicaciones.","title":"Intro"},{"location":"pages/kubernetes/architecture/k8s/#responsabilidades","text":"Kubelet es el responsable de todo lo que sucede en un worker node. Lo primero que tiene que hacer es registrar el nodo en el cluster, para eso crea un Kubernetes Node Resources usando el API Server. A partir de ese momento monitoriza todos los Pods para ver cuales son asignados por el Scheduler para ser ejecutados en su nodo, y cuando esto sucede habla con el Container Runtime que haya configurado para que se ejecuten los containers del Pod. Tambi\u00e9n monitoriza constantemente los containers de los Pods que est\u00e1n ejecutandose en su Nodo para poder reportar su estatus, eventos y consumo de recursos al API Server. Kubelet tambien es el responsable de ejecutar los liveness probes, rearrancar los containers cuando la probe falle y por \u00faltimo ser quien borre los contenedores cuando el Pod sea borrado.","title":"Responsabilidades"},{"location":"pages/kubernetes/architecture/k8s/#ejecutando-pods-staticos-sin-el-api-server","text":"Normalmente kubelet obtiene los manifiestos de los pods que tiene que ejecutar del API server, pero tambien puede ejecutar otros Pods obteniendo los manifiestos de un directorio del Nodo en el que se ejecuta. Esto se utiliza principalmente para poder ejecutar componentes del Control Plane como Pods en lugar de como procesos del master node.","title":"Ejecutando Pods staticos sin el API Server."},{"location":"pages/kubernetes/architecture/k8s/#kubernetes-service-proxy","text":"Los Worker Nodes tambien ejecutan el kube-proxy. El \u00fanico cometido de kube-proxy es asegurarse que cuando llamamos a uno de los Kubernetes Service la llamada termina en uno de los Pods que est\u00e1 dando soporte a dicho servicio. Cuando hay mas de un Pod dando soporte a dicho servicio se realiza un load-balancing. Se llama kube-proxy porque en la primera de las implementaciones era un proceso en espacio de usuario que hab\u00eda modificado las IP tables para que las peticiones a los servicios fueran redirigidas a \u00e9l y luego fuese el quien las reenviase a los Pods adecuados. Todas las conexiones pasaban por \u00e9l. Ahora hay una implementaci\u00f3n mucho mejor en performance que simplemente modifica la configuracion de las IP tables para que las conexiones vayan a parar al Pod directamente sin pasar por el proceso kube-proxy , por lo que ya no es un proxy estrictamente hablando, pero el nombre se le ha quedado. La diferencia principal entre los dos modos es que en el antiguo al tener que pasar todos los mensajes por el kube-proxy todos tenian que pasar por el espacio de usuario, lo que era una putada para el performance. Ademas el antiguo hac\u00eda un round robin entre los pods de un servicio mientras que el nuevo lo hace de manera aleatoria.","title":"Kubernetes Service Proxy"},{"location":"pages/kubernetes/architecture/k8s/#kubernetes-add-ons","text":"Hay algunos otros componentes que, sin ser extrictamente necesarios, permiten realizar acciones interesantes en el cluster. Son los add-ons. Estos componentes se instalan en el sistema como Pods, enviando sus manifiestos al API server como cualquier otro pod. Algunos de ellos se instalan mediante Deployment u otro tipo de recurso de k8s title: Algunos add-ons Podemos ver como en Minikube el Ingress Controller y el Dashboard son instalados mediante un [[Kubernetes ReplicationController|ReplicationController]] ```bash $ kubectl get rc -n kube-system NAME DESIRED CURRENT READY AGE default-http-backend 1 1 1 6d kubernetes-dashboard 1 1 1 6d nginx-ingress-controller 1 1 1 6d ``` Mientras que el DNS es un [Deployment](../../../Kubernetes%20Deployment \"Deployment\") ```bash $ kubectl get deploy -n kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-dns 1 1 1 1 6d","title":"Kubernetes Add-ons"},{"location":"pages/kubernetes/architecture/k8s/#como-funciona-el-dns-server","text":"Todos los pods en un cluster estan configurados para utilizar el DNS interno por defecto. El DNS se expone mediante un servicio kube-dns para poder asi mover los pods dentro del cluster sin problema. La direccion IP del servicio esta configurada como el nameserver en el fichero /etc/resolv.conf dentro de cada container instalado en el cluster. kube-dns utiliza el mecanismo de notificaciones del API server para observar cambios en los Kubernetes Service y en los Endpoints y actualiza sus tablas internas con cada uno de los cambios. Durante el momento en el que se actualiza un Kubernetes Service o un Kubernetes Endpoint hasta que el kube-dns recibe la notificacion y actualiza sus registros puede que haya informaci\u00f3n erronea en el DNS","title":"Como funciona el DNS server"},{"location":"pages/kubernetes/architecture/k8s/#ejemplo-de-como-funciona-todo-junto","text":"Todo kubernetes esta formado por peque\u00f1os elementos muy poco acoplados con una buena separacion de responsabilidades ( cada uno hace una cosa y solo una puta cosa) y esa es la raz\u00f3n por la que todo funciona bien Vamos a ver como funcionan todas las piezas en armon\u00eda para la creacion de un Pod. Normalmente no se crea un Pod desde cero, de modo que vamos a ver como se realiza la creacion de un Pod, pero partiendo del manifiesto de un Deployment. Antes siquiera de empezar ya tenemos la siguiente figura, donde cada uno de los componentes est\u00e1 escuchando las notificaciones adecuadas del API server respecto a los recursos que le son interesantes ( normalmente solo uno o un par a lo sumo) Bien, una vez que kubectl en via mediante un HTTP POST el manifiesto del deployment al API Server esta es la cadena de acontecimientos que se desata. Se pueden ver todos los eventos que van sucediendo en el cluster mediante el siguiente comando de kubectl kubectl get events --watch","title":"Ejemplo de como funciona todo junto"},{"location":"pages/kubernetes/architecture/k8s/#entendiendo-que-pasa-cuando-se-ejecuta-un-pod","text":"Cuando ejecutamos un Pod en uno de los nodos no solo se ejecutan los containers que tenemos definidos en el pod, si no que ademas se ejecuta otro pod. Supongamos que solo tenemos un container en el pod para hacerlo mas sencillo. $ kubectl run nginx --image = nginx deployment \"nginx\" created Si vamos al nodo en el que se est\u00e1 ejecutando el pod podemos ver los siguientes contenedores ejecutandose. docker@minikubeVM:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED c917a6f3c3f7 nginx \"nginx -g 'daemon off\" 4 seconds ago 98b8bf797174 gcr.io/.../pause:3.0 \"/pause\" 7 seconds ago Podemos ver que hay un contenedor adicional ejecutandose Dicho contenedor no est\u00e1 ejecutando nada porque esta en pause Ademas podemos ver que se ha creado antes que el contenedor que teniamos definido en el pod Ese contenedor es parte de la infraestructura Todos los contenedores de un mismo pod comparten algunos linux namespaces como por ejemplo el espacio de red \u2026 Este contenedor es el que contiene todos los linux namespaces que son compartidos por todos los contenedores de un mismo pod. Todos los contenedores del mismo pod tienen que compartir los mismos linux namespaces aunque se reinicien, pero eso es posible porque estos est\u00e1n en este container, de manera que aunque los dem\u00e1s se reinicien los namespaces se mantienen. El ciclo de vida de este contenedor esta ligado intimamente al del pod. Si por cualquier motivo este container resultase parado, kubelet volver\u00eda a recrearlo y despu\u00e9s recrear\u00eda todos los demas containers del pod para poder asignarles los linux namespaces del nuevo container.","title":"Entendiendo que pasa cuando se ejecuta un POD"},{"location":"pages/kubernetes/architecture/k8s/#interpod-networking","text":"Cada pod tiene un IP unica Todos los pods pueden comunicarse entre si mediante conexiones \u201cplanas\u201d es decir sin NAT Kubernetes no es quien configura esto, es el administrador quien tiene que conseguirlo. Kubernetes solo pone el requisito. La red tiene que estar configurada de tal manera que la IP que un Pod ve de si mismo sea la misma que los dem\u00e1s pueden ver de \u00e9l. Todos los containers deben de poder comunicarse entre ellos independientemente de si estan siendo ejecutados en el mismo o en distintos nodos.","title":"Interpod networking"}]}